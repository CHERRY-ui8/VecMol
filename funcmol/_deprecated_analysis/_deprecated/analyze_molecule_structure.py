"""
åˆ†æç”Ÿæˆåˆ†å­çš„ç»“æ„é—®é¢˜ï¼š
1. æœ€è¿‘åŸå­è·ç¦»åˆ†å¸ƒ
2. åˆ†å­è¿é€šæ€§ï¼ˆæ˜¯å¦å½¢æˆå®Œæ•´graphï¼‰
3. é”®é•¿åˆ†å¸ƒ
"""

import sys
from pathlib import Path
import torch
import numpy as np
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from funcmol.analysis.baselines_evaluation import (
    atom_decoder_dict,
    build_xae_molecule,
    bonds1,
    margin1
)


def compute_min_distances(coords):
    """
    è®¡ç®—åˆ†å­ä¸­æ‰€æœ‰åŸå­å¯¹çš„æœ€å°è·ç¦»
    
    Args:
        coords: [N, 3] åŸå­åæ ‡
        
    Returns:
        min_distances: æ¯ä¸ªåŸå­åˆ°æœ€è¿‘é‚»åŸå­çš„è·ç¦» [N]
        all_distances: æ‰€æœ‰åŸå­å¯¹çš„è·ç¦»ï¼ˆä¸Šä¸‰è§’çŸ©é˜µï¼‰
    """
    n_atoms = coords.shape[0]
    if n_atoms < 2:
        return torch.tensor([]), torch.tensor([])
    
    # è®¡ç®—æ‰€æœ‰åŸå­å¯¹çš„è·ç¦»
    distances = torch.cdist(coords, coords, p=2)  # [N, N]
    
    # å°†å¯¹è§’çº¿è®¾ä¸ºæ— ç©·å¤§ï¼ˆè‡ªå·±åˆ°è‡ªå·±çš„è·ç¦»ï¼‰
    distances.fill_diagonal_(float('inf'))
    
    # æ‰¾åˆ°æ¯ä¸ªåŸå­åˆ°æœ€è¿‘é‚»çš„è·ç¦»
    min_distances, _ = torch.min(distances, dim=1)
    
    # è·å–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    triu_mask = torch.triu(torch.ones(n_atoms, n_atoms, dtype=torch.bool), diagonal=1)
    all_distances = distances[triu_mask]
    
    return min_distances, all_distances


def check_connectivity(bond_types):
    """
    æ£€æŸ¥åˆ†å­çš„è¿é€šæ€§ï¼ˆä½¿ç”¨ç®€å•çš„DFSï¼‰
    
    Args:
        bond_types: [N, N] é”®ç±»å‹çŸ©é˜µ
        
    Returns:
        num_components: è¿é€šåˆ†é‡æ•°
        is_connected: æ˜¯å¦è¿é€šï¼ˆè¿é€šåˆ†é‡æ•°==1ï¼‰
    """
    n_atoms = bond_types.shape[0]
    if n_atoms == 0:
        return 0, False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¾¹
    has_edges = (bond_types > 0).any()
    if not has_edges:
        # æ²¡æœ‰è¾¹ï¼Œæ¯ä¸ªåŸå­éƒ½æ˜¯ç‹¬ç«‹çš„è¿é€šåˆ†é‡
        return n_atoms, False
    
    # ä½¿ç”¨DFSè®¡ç®—è¿é€šåˆ†é‡
    visited = torch.zeros(n_atoms, dtype=torch.bool)
    num_components = 0
    
    def dfs(node):
        """æ·±åº¦ä¼˜å…ˆæœç´¢"""
        visited[node] = True
        # æ‰¾åˆ°æ‰€æœ‰ä¸å½“å‰èŠ‚ç‚¹ç›¸è¿çš„èŠ‚ç‚¹
        neighbors = torch.nonzero(bond_types[node] > 0, as_tuple=False).squeeze(-1)
        for neighbor in neighbors:
            if neighbor.item() != node and not visited[neighbor.item()]:
                dfs(neighbor.item())
    
    # éå†æ‰€æœ‰æœªè®¿é—®çš„èŠ‚ç‚¹
    for i in range(n_atoms):
        if not visited[i]:
            dfs(i)
            num_components += 1
    
    is_connected = (num_components == 1)
    return num_components, is_connected


def compute_missing_bond_deviations(positions, atom_types, bond_types, atom_decoder, dataset_info, margin1_val=40):
    """
    è®¡ç®—åº”è¯¥å½¢æˆé”®ä½†æœªå½¢æˆé”®çš„åŸå­å¯¹çš„è·ç¦»åå·®
    
    Args:
        positions: [N, 3] åŸå­åæ ‡
        atom_types: [N] åŸå­ç±»å‹ç´¢å¼•
        bond_types: [N, N] é”®ç±»å‹çŸ©é˜µ
        atom_decoder: åŸå­ç±»å‹è§£ç å™¨åˆ—è¡¨
        dataset_info: æ•°æ®é›†ä¿¡æ¯å­—å…¸
        margin1_val: margin1 å€¼ï¼ˆpmå•ä½ï¼‰ï¼Œ
    
    Returns:
        missing_bonds: List of dicts with keys: pair, actual_dist, standard_dist, deviation_pct
    """
    n = positions.shape[0]
    dists = torch.cdist(positions, positions, p=2)
    missing_bonds = []
    
    for i in range(n):
        for j in range(i):
            if bond_types[i, j] == 0:  # å½“å‰åˆ¤æ–­ä¸ºæ— é”®
                atom1_str = atom_decoder[atom_types[i].item()]
                atom2_str = atom_decoder[atom_types[j].item()]
                
                # æ£€æŸ¥æ ‡å‡†é”®é•¿ï¼ˆéœ€è¦æŒ‰å­—æ¯é¡ºåºæ’åºï¼‰
                pair = sorted([atom1_str, atom2_str])
                atom1_key = pair[0]
                atom2_key = pair[1]
                
                # æ£€æŸ¥æ ‡å‡†é”®é•¿
                if atom1_key in bonds1 and atom2_key in bonds1[atom1_key]:
                    standard_dist_pm = bonds1[atom1_key][atom2_key]
                    standard_dist = standard_dist_pm / 100.0  # è½¬æ¢ä¸ºÃ…
                    threshold = (standard_dist_pm + margin1_val) / 100.0  # è½¬æ¢ä¸ºÃ…
                    actual_dist = dists[i, j].item()
                    
                    # å¦‚æœè·ç¦»åœ¨åº”è¯¥å½¢æˆé”®çš„èŒƒå›´å†…
                    if actual_dist < threshold:
                        deviation_pct = (actual_dist - standard_dist) / standard_dist * 100
                        missing_bonds.append({
                            'pair': (i, j),
                            'actual_dist': actual_dist,
                            'standard_dist': standard_dist,
                            'deviation_pct': deviation_pct,
                            'atom1': atom1_str,
                            'atom2': atom2_str
                        })
    
    return missing_bonds


def compute_missing_bond_deviations_strict(positions, atom_types, bond_types, atom_decoder, dataset_info, strict_margin=15):
    """
    è®¡ç®—åº”è¯¥å½¢æˆé”®ä½†æœªå½¢æˆé”®çš„åŸå­å¯¹çš„è·ç¦»åå·®ï¼ˆä½¿ç”¨ä¸¥æ ¼æ ‡å‡†ï¼‰
    
    æ£€æŸ¥ä¸¤ç§æƒ…å†µï¼š
    1. è·ç¦»åœ¨ä¸¥æ ¼é˜ˆå€¼å†…ï¼Œä½†æ²¡æœ‰é”®ï¼ˆbond_types[i, j] == 0ï¼‰
    2. è·ç¦»åœ¨ä¸¥æ ¼é˜ˆå€¼å†…ï¼Œæœ‰é”®ï¼Œä½†å±äºä¸åŒçš„è¿é€šåˆ†é‡ï¼ˆè·¨åˆ†é‡æ–­è£‚ï¼‰
    
    Args:
        positions: [N, 3] åŸå­åæ ‡
        atom_types: [N] åŸå­ç±»å‹ç´¢å¼•
        bond_types: [N, N] é”®ç±»å‹çŸ©é˜µ
        atom_decoder: åŸå­ç±»å‹è§£ç å™¨åˆ—è¡¨
        dataset_info: æ•°æ®é›†ä¿¡æ¯å­—å…¸
        strict_margin: ä¸¥æ ¼marginå€¼ï¼ˆpmå•ä½ï¼‰ï¼Œé»˜è®¤15pm
    
    Returns:
        missing_bonds: List of dicts with keys: pair, actual_dist, standard_dist, deviation_pct
    """
    n = positions.shape[0]
    dists = torch.cdist(positions, positions, p=2)
    missing_bonds = []
    
    # è®¡ç®—è¿é€šåˆ†é‡ï¼Œç”¨äºæ£€æµ‹è·¨åˆ†é‡çš„ç¼ºå¤±é”®
    num_components, component_ids = check_connectivity_with_labels(bond_types)
    
    for i in range(n):
        for j in range(i):
            atom1_str = atom_decoder[atom_types[i].item()]
            atom2_str = atom_decoder[atom_types[j].item()]
            
            # æ£€æŸ¥æ ‡å‡†é”®é•¿ï¼ˆéœ€è¦æŒ‰å­—æ¯é¡ºåºæ’åºï¼‰
            pair = sorted([atom1_str, atom2_str])
            atom1_key = pair[0]
            atom2_key = pair[1]
            
            # æ£€æŸ¥æ ‡å‡†é”®é•¿
            if atom1_key in bonds1 and atom2_key in bonds1[atom1_key]:
                standard_dist_pm = bonds1[atom1_key][atom2_key]
                standard_dist = standard_dist_pm / 100.0  # è½¬æ¢ä¸ºÃ…
                strict_threshold = (standard_dist_pm + strict_margin) / 100.0  # ä¸¥æ ¼é˜ˆå€¼
                # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼æ¥æ£€æµ‹åç¦»æ ‡å‡†å€¼çš„é”®ï¼ˆæ ‡å‡†é”®é•¿ + 30pmï¼‰
                relaxed_threshold = (standard_dist_pm + 30) / 100.0
                actual_dist = dists[i, j].item()
                
                # æ£€æŸ¥ä¸¤ç§æƒ…å†µï¼š
                # 1. è·ç¦»åœ¨ä¸¥æ ¼é˜ˆå€¼å†…ï¼ˆåº”è¯¥å½¢æˆé”®ï¼‰
                # 2. è·ç¦»åœ¨å®½æ¾é˜ˆå€¼å†…ä½†æœ‰é”®ï¼Œä¸”åç¦»æ ‡å‡†å€¼è¶…è¿‡5%ï¼ˆç»“æ„ä¸ç†æƒ³ï¼‰
                is_cross_component = (component_ids[i] != component_ids[j])
                has_bond = (bond_types[i, j] > 0)
                deviation_pct = (actual_dist - standard_dist) / standard_dist * 100
                significant_deviation = abs(deviation_pct) > 5.0
                
                # æƒ…å†µ1ï¼šè·ç¦»åœ¨ä¸¥æ ¼é˜ˆå€¼å†…ï¼Œåº”è¯¥å½¢æˆé”®
                if actual_dist < strict_threshold:
                    if bond_types[i, j] == 0 or is_cross_component:
                        missing_bonds.append({
                            'pair': (i, j),
                            'actual_dist': actual_dist,
                            'standard_dist': standard_dist,
                            'deviation_pct': deviation_pct,
                            'atom1': atom1_str,
                            'atom2': atom2_str,
                            'is_cross_component': is_cross_component,
                            'has_bond': has_bond
                        })
                # æƒ…å†µ2ï¼šè·ç¦»åœ¨å®½æ¾é˜ˆå€¼å†…ï¼Œæœ‰é”®ä½†åç¦»æ ‡å‡†å€¼ï¼ˆç»“æ„ä¸ç†æƒ³ï¼‰
                elif actual_dist < relaxed_threshold and has_bond and significant_deviation:
                    missing_bonds.append({
                        'pair': (i, j),
                        'actual_dist': actual_dist,
                        'standard_dist': standard_dist,
                        'deviation_pct': deviation_pct,
                        'atom1': atom1_str,
                        'atom2': atom2_str,
                        'is_cross_component': is_cross_component,
                        'has_bond': has_bond
                    })
    
    return missing_bonds


def check_connectivity_with_labels(bond_types):
    """
    æ£€æŸ¥åˆ†å­çš„è¿é€šæ€§å¹¶è¿”å›æ¯ä¸ªåŸå­æ‰€å±çš„è¿é€šåˆ†é‡ID
    
    Args:
        bond_types: [N, N] é”®ç±»å‹çŸ©é˜µ
        
    Returns:
        num_components: è¿é€šåˆ†é‡æ•°
        component_ids: æ¯ä¸ªåŸå­æ‰€å±çš„è¿é€šåˆ†é‡ID [N]
    """
    n_atoms = bond_types.shape[0]
    if n_atoms == 0:
        return 0, torch.zeros(0, dtype=torch.long)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¾¹
    has_edges = (bond_types > 0).any()
    if not has_edges:
        # æ²¡æœ‰è¾¹ï¼Œæ¯ä¸ªåŸå­éƒ½æ˜¯ç‹¬ç«‹çš„è¿é€šåˆ†é‡
        component_ids = torch.arange(n_atoms, dtype=torch.long)
        return n_atoms, component_ids
    
    # ä½¿ç”¨DFSè®¡ç®—è¿é€šåˆ†é‡å¹¶æ ‡è®°
    visited = torch.zeros(n_atoms, dtype=torch.bool)
    component_ids = torch.zeros(n_atoms, dtype=torch.long)
    current_component = 0
    
    def dfs_label(node, comp_id):
        """æ·±åº¦ä¼˜å…ˆæœç´¢å¹¶æ ‡è®°è¿é€šåˆ†é‡"""
        visited[node] = True
        component_ids[node] = comp_id
        neighbors = torch.nonzero(bond_types[node] > 0, as_tuple=False).squeeze(-1)
        for neighbor in neighbors:
            if neighbor.item() != node and not visited[neighbor.item()]:
                dfs_label(neighbor.item(), comp_id)
    
    # éå†æ‰€æœ‰æœªè®¿é—®çš„èŠ‚ç‚¹
    for i in range(n_atoms):
        if not visited[i]:
            dfs_label(i, current_component)
            current_component += 1
    
    num_components = current_component
    return num_components, component_ids


def compute_excessive_bond_deviations(positions, atom_types, bond_types, atom_decoder, dataset_info, margin1_val=40):
    """
    è®¡ç®—ä¸åº”è¯¥å½¢æˆé”®ä½†å½¢æˆé”®çš„åŸå­å¯¹ï¼ˆè·ç¦»è¿‡è¿œä½†ä»è¢«åˆ¤æ–­ä¸ºæœ‰é”®ï¼‰
    
    Args:
        positions: [N, 3] åŸå­åæ ‡
        atom_types: [N] åŸå­ç±»å‹ç´¢å¼•
        bond_types: [N, N] é”®ç±»å‹çŸ©é˜µ
        atom_decoder: åŸå­ç±»å‹è§£ç å™¨åˆ—è¡¨
        dataset_info: æ•°æ®é›†ä¿¡æ¯å­—å…¸
        margin1_val: margin1 å€¼ï¼ˆpmå•ä½ï¼‰ï¼Œ
    
    Returns:
        excessive_bonds: List of dicts with keys: pair, actual_dist, standard_dist, deviation_pct
    """
    n = positions.shape[0]
    dists = torch.cdist(positions, positions, p=2)
    excessive_bonds = []
    
    for i in range(n):
        for j in range(i):
            if bond_types[i, j] > 0:  # å½“å‰åˆ¤æ–­ä¸ºæœ‰é”®
                atom1_str = atom_decoder[atom_types[i].item()]
                atom2_str = atom_decoder[atom_types[j].item()]
                
                # æ£€æŸ¥æ ‡å‡†é”®é•¿ï¼ˆéœ€è¦æŒ‰å­—æ¯é¡ºåºæ’åºï¼‰
                pair = sorted([atom1_str, atom2_str])
                atom1_key = pair[0]
                atom2_key = pair[1]
                
                # æ£€æŸ¥æ ‡å‡†é”®é•¿
                if atom1_key in bonds1 and atom2_key in bonds1[atom1_key]:
                    standard_dist_pm = bonds1[atom1_key][atom2_key]
                    standard_dist = standard_dist_pm / 100.0  # è½¬æ¢ä¸ºÃ…
                    threshold = (standard_dist_pm + margin1_val) / 100.0  # è½¬æ¢ä¸ºÃ…
                    actual_dist = dists[i, j].item()
                    
                    # å¦‚æœè·ç¦»è¶…è¿‡åº”è¯¥å½¢æˆé”®çš„èŒƒå›´
                    if actual_dist > threshold:
                        deviation_pct = (actual_dist - standard_dist) / standard_dist * 100
                        excessive_bonds.append({
                            'pair': (i, j),
                            'actual_dist': actual_dist,
                            'standard_dist': standard_dist,
                            'deviation_pct': deviation_pct,
                            'atom1': atom1_str,
                            'atom2': atom2_str,
                            'bond_order': bond_types[i, j].item()
                        })
    
    return excessive_bonds


def compute_connectivity_continuity_score(positions, atom_types, bond_types, atom_decoder, dataset_info, margin1_val=40):
    """
    è®¡ç®—ç»¼åˆè¿ç»­æ€§è¿é€šæ€§åˆ†æ•°
    
    ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼ˆæ ‡å‡†é”®é•¿ + 15pmï¼‰æ¥åˆ¤æ–­"åº”è¯¥å½¢æˆé”®"ï¼Œ
    è¿™æ ·å¯ä»¥æ£€æµ‹å‡ºå³ä½¿ä½¿ç”¨å®½æ¾marginåˆ¤æ–­ä¸ºæœ‰é”®ï¼Œä½†è·ç¦»ä»ç„¶åç¦»æ ‡å‡†å€¼çš„æƒ…å†µã€‚
    
    Args:
        positions: [N, 3] åŸå­åæ ‡
        atom_types: [N] åŸå­ç±»å‹ç´¢å¼•
        bond_types: [N, N] é”®ç±»å‹çŸ©é˜µ
        atom_decoder: åŸå­ç±»å‹è§£ç å™¨åˆ—è¡¨
        dataset_info: æ•°æ®é›†ä¿¡æ¯å­—å…¸
        margin1_val: margin1 å€¼ï¼ˆpmå•ä½ï¼‰ï¼Œç”¨äºé”®åˆ¤æ–­ï¼Œä½†è¿ç»­æ€§è¯„ä¼°ä½¿ç”¨æ›´ä¸¥æ ¼æ ‡å‡†
    
    Returns:
        dict: åŒ…å«è¿ç»­æ€§æŒ‡æ ‡çš„å­—å…¸
    """
    n = positions.shape[0]
    dists = torch.cdist(positions, positions, p=2)
    
    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ ‡å‡†æ¥åˆ¤æ–­"åº”è¯¥å½¢æˆé”®"ï¼šæ ‡å‡†é”®é•¿ + 15pm
    # è¿™æ ·å¯ä»¥æ£€æµ‹å‡ºè·ç¦»åç¦»æ ‡å‡†å€¼çš„æƒ…å†µï¼Œå³ä½¿å®ƒä»¬è¢«å®½æ¾çš„marginåˆ¤æ–­ä¸ºæœ‰é”®
    strict_margin = 15  # ä½¿ç”¨è¾ƒå°çš„marginæ¥å®šä¹‰"åº”è¯¥å½¢æˆé”®"çš„æ ‡å‡†
    
    missing_bonds = []
    all_potential_bonds = []  # æ‰€æœ‰åº”è¯¥å½¢æˆé”®çš„åŸå­å¯¹ï¼ˆæ— è®ºæ˜¯å¦å·²å½¢æˆé”®ï¼‰
    
    for i in range(n):
        for j in range(i):
            atom1_str = atom_decoder[atom_types[i].item()]
            atom2_str = atom_decoder[atom_types[j].item()]
            pair = sorted([atom1_str, atom2_str])
            atom1_key = pair[0]
            atom2_key = pair[1]
            
            if atom1_key in bonds1 and atom2_key in bonds1[atom1_key]:
                standard_dist_pm = bonds1[atom1_key][atom2_key]
                standard_dist = standard_dist_pm / 100.0  # è½¬æ¢ä¸ºÃ…
                strict_threshold = (standard_dist_pm + strict_margin) / 100.0  # ä¸¥æ ¼é˜ˆå€¼
                actual_dist = dists[i, j].item()
                
                # å¦‚æœè·ç¦»åœ¨"åº”è¯¥å½¢æˆé”®"çš„èŒƒå›´å†…ï¼ˆä½¿ç”¨ä¸¥æ ¼æ ‡å‡†ï¼‰
                if actual_dist < strict_threshold:
                    deviation_pct = (actual_dist - standard_dist) / standard_dist * 100
                    all_potential_bonds.append({
                        'pair': (i, j),
                        'actual_dist': actual_dist,
                        'standard_dist': standard_dist,
                        'deviation_pct': deviation_pct,
                        'has_bond': bond_types[i, j] > 0
                    })
                    
                    # å¦‚æœå½“å‰åˆ¤æ–­ä¸ºæ— é”®ï¼Œè®°å½•ä¸ºç¼ºå¤±é”®
                    if bond_types[i, j] == 0:
                        missing_bonds.append({
                            'pair': (i, j),
                            'actual_dist': actual_dist,
                            'standard_dist': standard_dist,
                            'deviation_pct': deviation_pct,
                            'atom1': atom1_str,
                            'atom2': atom2_str
                        })
    
    if len(all_potential_bonds) == 0:
        return {
            'mean_deviation_pct': 0.0,
            'max_deviation_pct': 0.0,
            'missing_bond_count': 0,
            'missing_bond_ratio': 0.0,
            'continuity_score': 1.0,
            'overall_mean_deviation_pct': 0.0
        }
    
    # è®¡ç®—æ‰€æœ‰åº”è¯¥å½¢æˆé”®çš„åŸå­å¯¹çš„åå·®ï¼ˆåŒ…æ‹¬å·²å½¢æˆé”®çš„ï¼‰
    all_deviations = [bond['deviation_pct'] for bond in all_potential_bonds]
    overall_mean_deviation = np.mean(all_deviations)
    overall_max_deviation = np.max(all_deviations)
    
    # è®¡ç®—ç¼ºå¤±é”®çš„ç»Ÿè®¡ä¿¡æ¯
    if len(missing_bonds) > 0:
        missing_deviations = [bond['deviation_pct'] for bond in missing_bonds]
        mean_deviation = np.mean(missing_deviations)
        max_deviation = np.max(missing_deviations)
    else:
        mean_deviation = 0.0
        max_deviation = 0.0
    
    missing_bond_ratio = len(missing_bonds) / len(all_potential_bonds) if len(all_potential_bonds) > 0 else 0.0
    
    # è®¡ç®—è¿ç»­æ€§åˆ†æ•°ï¼ˆç»¼åˆè€ƒè™‘æ‰€æœ‰åº”è¯¥å½¢æˆé”®çš„åŸå­å¯¹çš„åå·®ï¼‰
    # ä½¿ç”¨æ•´ä½“å¹³å‡åå·®çš„å½’ä¸€åŒ–ç‰ˆæœ¬
    # å‡è®¾æœ€å¤§åˆç†åå·®ä¸º30%ï¼Œè¶…è¿‡30%è®¤ä¸ºä¸¥é‡åç¦»
    normalized_deviation = min(abs(overall_mean_deviation) / 30.0, 1.0)
    continuity_score = 1.0 - normalized_deviation
    
    return {
        'mean_deviation_pct': mean_deviation,
        'max_deviation_pct': max_deviation,
        'missing_bond_count': len(missing_bonds),
        'missing_bond_ratio': missing_bond_ratio,
        'continuity_score': continuity_score,
        'total_potential_bonds': len(all_potential_bonds),
        'overall_mean_deviation_pct': overall_mean_deviation,
        'overall_max_deviation_pct': overall_max_deviation
    }


def analyze_molecules(molecule_dir, output_dir=None):
    """
    åˆ†æåˆ†å­ç»“æ„
    
    Args:
        molecule_dir: åŒ…å« .npz æ–‡ä»¶çš„ç›®å½•
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    """
    molecule_dir = Path(molecule_dir)
    npz_files = sorted(molecule_dir.glob("generated_*.npz"))
    
    print(f"æ‰¾åˆ° {len(npz_files)} ä¸ª .npz åˆ†å­æ–‡ä»¶")
    
    atom_decoder = atom_decoder_dict['qm9_with_h']
    dataset_info = {'name': 'qm9'}
    
    # å­˜å‚¨ç»Ÿè®¡æ•°æ®
    all_min_distances = []
    all_pair_distances = []
    num_components_list = []
    is_connected_list = []
    bond_lengths = []
    num_atoms_list = []
    
    # ç©ºé—´åˆ†å¸ƒç»Ÿè®¡
    coord_ranges = []  # æ¯ä¸ªåˆ†å­çš„åæ ‡èŒƒå›´ (max - min)
    coord_centers = []  # æ¯ä¸ªåˆ†å­çš„ä¸­å¿ƒåæ ‡
    coord_spans = []  # æ¯ä¸ªåˆ†å­çš„è·¨åº¦ï¼ˆæœ€å¤§è·ç¦»ï¼‰
    large_gap_ratios = []  # æ¯ä¸ªåˆ†å­ä¸­è·ç¦»>3Ã…çš„åŸå­å¯¹æ¯”ä¾‹
    
    # è¿ç»­æ€§æŒ‡æ ‡ç»Ÿè®¡
    all_missing_bond_deviations = []  # æ‰€æœ‰ç¼ºå¤±é”®çš„åå·®ç™¾åˆ†æ¯”
    all_continuity_scores = []  # æ‰€æœ‰åˆ†å­çš„è¿ç»­æ€§åˆ†æ•°
    all_missing_bond_ratios = []  # æ‰€æœ‰åˆ†å­çš„ç¼ºå¤±é”®æ¯”ä¾‹
    all_mean_deviations = []  # æ‰€æœ‰åˆ†å­çš„å¹³å‡åå·®ï¼ˆä»…ç¼ºå¤±é”®ï¼‰
    all_max_deviations = []  # æ‰€æœ‰åˆ†å­çš„æœ€å¤§åå·®ï¼ˆä»…ç¼ºå¤±é”®ï¼‰
    all_overall_mean_deviations = []  # æ‰€æœ‰åˆ†å­çš„æ•´ä½“å¹³å‡åå·®ï¼ˆæ‰€æœ‰åº”è¯¥å½¢æˆé”®çš„åŸå­å¯¹ï¼‰
    all_overall_max_deviations = []  # æ‰€æœ‰åˆ†å­çš„æ•´ä½“æœ€å¤§åå·®
    
    print("åˆ†æåˆ†å­ç»“æ„...")
    for npz_file in tqdm(npz_files, desc="å¤„ç†åˆ†å­"):
        try:
            # åŠ è½½åˆ†å­
            data = np.load(npz_file)
            coords = data['coords']  # (N, 3)
            types = data['types']    # (N,)
            
            # è½¬æ¢ä¸ºtorchå¼ é‡
            positions = torch.tensor(coords, dtype=torch.float32)
            atom_types = torch.tensor(types, dtype=torch.long)
            
            # è¿‡æ»¤æ‰å¡«å……çš„åŸå­
            valid_mask = atom_types != -1
            if not valid_mask.any():
                continue
            
            positions = positions[valid_mask]
            atom_types = atom_types[valid_mask]
            num_atoms = len(positions)
            num_atoms_list.append(num_atoms)
            
            # è®¡ç®—ç©ºé—´åˆ†å¸ƒç»Ÿè®¡
            coord_min = positions.min(dim=0)[0]
            coord_max = positions.max(dim=0)[0]
            coord_range = coord_max - coord_min  # [3]
            coord_center = positions.mean(dim=0)  # [3]
            coord_span = torch.cdist(positions, positions, p=2).max().item()  # æœ€å¤§åŸå­å¯¹è·ç¦»
            
            coord_ranges.append(coord_range.cpu().numpy())
            coord_centers.append(coord_center.cpu().numpy())
            coord_spans.append(coord_span)
            
            # è®¡ç®—æœ€è¿‘è·ç¦»
            min_dists, pair_dists = compute_min_distances(positions)
            if len(min_dists) > 0:
                all_min_distances.extend(min_dists.cpu().numpy())
            if len(pair_dists) > 0:
                all_pair_distances.extend(pair_dists.cpu().numpy())
                # è®¡ç®—è·ç¦»>3Ã…çš„åŸå­å¯¹æ¯”ä¾‹
                large_gap_ratio = (pair_dists > 3.0).sum().item() / len(pair_dists) * 100
                large_gap_ratios.append(large_gap_ratio)
            
            # æ„å»ºé”®ç±»å‹çŸ©é˜µ
            _, _, bond_types = build_xae_molecule(
                positions=positions,
                atom_types=atom_types,
                dataset_info=dataset_info,
                atom_decoder=atom_decoder
            )
            
            # æ£€æŸ¥è¿é€šæ€§
            num_components, is_connected = check_connectivity(bond_types)
            num_components_list.append(num_components)
            is_connected_list.append(is_connected)
            
            # è®¡ç®—è¿ç»­æ€§æŒ‡æ ‡ï¼ˆå†…éƒ¨ä½¿ç”¨ä¸¥æ ¼æ ‡å‡†15pmæ¥åˆ¤æ–­"åº”è¯¥å½¢æˆé”®"ï¼‰
            continuity_metrics = compute_connectivity_continuity_score(
                positions, atom_types, bond_types, atom_decoder, dataset_info, margin1_val=margin1
            )
            all_continuity_scores.append(continuity_metrics['continuity_score'])
            all_missing_bond_ratios.append(continuity_metrics['missing_bond_ratio'])
            all_mean_deviations.append(continuity_metrics['mean_deviation_pct'])
            all_max_deviations.append(continuity_metrics['max_deviation_pct'])
            all_overall_mean_deviations.append(continuity_metrics['overall_mean_deviation_pct'])
            all_overall_max_deviations.append(continuity_metrics['overall_max_deviation_pct'])
            
            # é‡æ–°è®¡ç®—ç¼ºå¤±é”®ï¼ˆä½¿ç”¨ä¸è¿ç»­æ€§è¯„ä¼°ç›¸åŒçš„ä¸¥æ ¼æ ‡å‡†15pmï¼‰
            # è¿™æ ·å¯ä»¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            missing_bonds = compute_missing_bond_deviations_strict(
                positions, atom_types, bond_types, atom_decoder, dataset_info, strict_margin=15
            )
            if missing_bonds:
                missing_deviations = [bond['deviation_pct'] for bond in missing_bonds]
                all_missing_bond_deviations.extend(missing_deviations)
            
            # è®¡ç®—é”®é•¿ï¼ˆåªè®¡ç®—æœ‰é”®çš„åŸå­å¯¹ï¼‰
            distances = torch.cdist(positions, positions, p=2)
            triu_mask = torch.triu(torch.ones_like(bond_types, dtype=torch.bool), diagonal=1)
            bond_mask = (bond_types > 0) & triu_mask
            
            if bond_mask.any():
                bond_distances = distances[bond_mask]
                bond_lengths.extend(bond_distances.cpu().numpy())
            
        except Exception as e:
            print(f"\nå¤„ç†æ–‡ä»¶ {npz_file} æ—¶å‡ºé”™: {e}")
            continue
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_min_distances = np.array(all_min_distances)
    all_pair_distances = np.array(all_pair_distances)
    num_components_list = np.array(num_components_list)
    is_connected_list = np.array(is_connected_list)
    bond_lengths = np.array(bond_lengths) if bond_lengths else np.array([])
    num_atoms_list = np.array(num_atoms_list)
    coord_ranges = np.array(coord_ranges)  # [N_mols, 3]
    coord_centers = np.array(coord_centers)  # [N_mols, 3]
    coord_spans = np.array(coord_spans)
    large_gap_ratios = np.array(large_gap_ratios) if large_gap_ratios else np.array([])
    
    # è¿ç»­æ€§æŒ‡æ ‡æ•°ç»„
    all_continuity_scores = np.array(all_continuity_scores) if all_continuity_scores else np.array([])
    all_missing_bond_ratios = np.array(all_missing_bond_ratios) if all_missing_bond_ratios else np.array([])
    all_mean_deviations = np.array(all_mean_deviations) if all_mean_deviations else np.array([])
    all_max_deviations = np.array(all_max_deviations) if all_max_deviations else np.array([])
    all_missing_bond_deviations = np.array(all_missing_bond_deviations) if all_missing_bond_deviations else np.array([])
    all_overall_mean_deviations = np.array(all_overall_mean_deviations) if all_overall_mean_deviations else np.array([])
    all_overall_max_deviations = np.array(all_overall_max_deviations) if all_overall_max_deviations else np.array([])
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "="*60)
    print("åˆ†å­ç»“æ„åˆ†æç»“æœ")
    print("="*60)
    
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»åˆ†å­æ•°: {len(npz_files)}")
    print(f"  å¹³å‡åŸå­æ•°: {num_atoms_list.mean():.2f}")
    print(f"  åŸå­æ•°èŒƒå›´: {num_atoms_list.min()} - {num_atoms_list.max()}")
    
    print(f"\nğŸ“ æœ€è¿‘åŸå­è·ç¦»ç»Ÿè®¡:")
    if len(all_min_distances) > 0:
        print(f"  å¹³å‡æœ€è¿‘è·ç¦»: {all_min_distances.mean():.4f} Ã…")
        print(f"  ä¸­ä½æ•°æœ€è¿‘è·ç¦»: {np.median(all_min_distances):.4f} Ã…")
        print(f"  æœ€å°æœ€è¿‘è·ç¦»: {all_min_distances.min():.4f} Ã…")
        print(f"  æœ€å¤§æœ€è¿‘è·ç¦»: {all_min_distances.max():.4f} Ã…")
        print(f"  æ ‡å‡†å·®: {all_min_distances.std():.4f} Ã…")
        
        # ç»Ÿè®¡è·ç¦»è¿‡å¤§çš„åŸå­æ¯”ä¾‹
        large_dist_threshold = 3.0  # è¶…è¿‡3Ã…è®¤ä¸ºè·ç¦»è¿‡å¤§
        large_dist_ratio = (all_min_distances > large_dist_threshold).sum() / len(all_min_distances) * 100
        print(f"  æœ€è¿‘è·ç¦» > {large_dist_threshold}Ã… çš„åŸå­æ¯”ä¾‹: {large_dist_ratio:.2f}%")
        
        # ç»Ÿè®¡è·ç¦»è¿‡å°çš„åŸå­æ¯”ä¾‹ï¼ˆå¯èƒ½æ˜¯é‡å ï¼‰
        small_dist_threshold = 0.5  # å°äº0.5Ã…è®¤ä¸ºè·ç¦»è¿‡å°
        small_dist_ratio = (all_min_distances < small_dist_threshold).sum() / len(all_min_distances) * 100
        print(f"  æœ€è¿‘è·ç¦» < {small_dist_threshold}Ã… çš„åŸå­æ¯”ä¾‹: {small_dist_ratio:.2f}%")
    
    print(f"\nğŸ”— é”®é•¿ç»Ÿè®¡:")
    if len(bond_lengths) > 0:
        print(f"  æ€»é”®æ•°: {len(bond_lengths)}")
        print(f"  å¹³å‡é”®é•¿: {bond_lengths.mean():.4f} Ã…")
        print(f"  ä¸­ä½æ•°é”®é•¿: {np.median(bond_lengths):.4f} Ã…")
        print(f"  é”®é•¿èŒƒå›´: {bond_lengths.min():.4f} - {bond_lengths.max():.4f} Ã…")
        
        # ç»Ÿè®¡å¼‚å¸¸é”®é•¿
        normal_bond_range = (0.7, 2.0)  # æ­£å¸¸é”®é•¿èŒƒå›´
        normal_bond_ratio = ((bond_lengths >= normal_bond_range[0]) & 
                            (bond_lengths <= normal_bond_range[1])).sum() / len(bond_lengths) * 100
        print(f"  æ­£å¸¸é”®é•¿ ({normal_bond_range[0]}-{normal_bond_range[1]}Ã…) æ¯”ä¾‹: {normal_bond_ratio:.2f}%")
    else:
        print("  æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•é”®ï¼")
    
    print(f"\nğŸŒ åˆ†å­è¿é€šæ€§ç»Ÿè®¡:")
    print(f"  è¿é€šåˆ†å­æ•°ï¼ˆè¿é€šåˆ†é‡=1ï¼‰: {is_connected_list.sum()}")
    print(f"  éè¿é€šåˆ†å­æ•°ï¼ˆè¿é€šåˆ†é‡>1ï¼‰: {(~is_connected_list).sum()}")
    print(f"  è¿é€šåˆ†å­æ¯”ä¾‹: {is_connected_list.sum() / len(is_connected_list) * 100:.2f}%")
    print(f"  å¹³å‡è¿é€šåˆ†é‡æ•°: {num_components_list.mean():.2f}")
    print(f"  æœ€å¤§è¿é€šåˆ†é‡æ•°: {num_components_list.max()}")
    print(f"  è¿é€šåˆ†é‡æ•°åˆ†å¸ƒ:")
    unique, counts = np.unique(num_components_list, return_counts=True)
    for comp, count in zip(unique, counts):
        print(f"    {comp} ä¸ªè¿é€šåˆ†é‡: {count} ä¸ªåˆ†å­ ({count/len(num_components_list)*100:.2f}%)")
    
    print(f"\nğŸ”— è¿ç»­æ€§è¿é€šæ€§æŒ‡æ ‡ (é”®åˆ¤æ–­ä½¿ç”¨ margin1={margin1}pm, è¿ç»­æ€§è¯„ä¼°ä½¿ç”¨ä¸¥æ ¼æ ‡å‡†: æ ‡å‡†é”®é•¿+15pm):")
    if len(all_continuity_scores) > 0:
        print(f"  å¹³å‡è¿ç»­æ€§åˆ†æ•°: {all_continuity_scores.mean():.4f} (1.0=å®Œç¾è¿é€š)")
        print(f"  ä¸­ä½æ•°è¿ç»­æ€§åˆ†æ•°: {np.median(all_continuity_scores):.4f}")
        print(f"  è¿ç»­æ€§åˆ†æ•°èŒƒå›´: {all_continuity_scores.min():.4f} - {all_continuity_scores.max():.4f}")
        print(f"  è¿ç»­æ€§åˆ†æ•°æ ‡å‡†å·®: {all_continuity_scores.std():.4f}")
        print(f"  è¯´æ˜: åˆ†æ•°åŸºäºæ‰€æœ‰åº”è¯¥å½¢æˆé”®çš„åŸå­å¯¹çš„æ•´ä½“åå·®è®¡ç®—ï¼ˆä½¿ç”¨ä¸¥æ ¼æ ‡å‡†: æ ‡å‡†é”®é•¿+15pmï¼‰")
        
        print(f"\n  æ•´ä½“åå·®ç»Ÿè®¡ï¼ˆæ‰€æœ‰åº”è¯¥å½¢æˆé”®çš„åŸå­å¯¹ï¼Œæ— è®ºæ˜¯å¦å·²å½¢æˆé”®ï¼‰:")
        if len(all_overall_mean_deviations) > 0:
            print(f"    å¹³å‡æ•´ä½“åå·®ç™¾åˆ†æ¯”: {all_overall_mean_deviations.mean():.4f}%")
            print(f"    ä¸­ä½æ•°æ•´ä½“åå·®ç™¾åˆ†æ¯”: {np.median(all_overall_mean_deviations):.4f}%")
            print(f"    æ•´ä½“åå·®èŒƒå›´: {all_overall_mean_deviations.min():.4f}% - {all_overall_mean_deviations.max():.4f}%")
            print(f"    æ•´ä½“åå·®æ ‡å‡†å·®: {all_overall_mean_deviations.std():.4f}%")
        
        if len(all_overall_max_deviations) > 0:
            print(f"    æœ€å¤§æ•´ä½“åå·®ç™¾åˆ†æ¯”ï¼ˆå¹³å‡ï¼‰: {all_overall_max_deviations.mean():.4f}%")
            print(f"    æœ€å¤§æ•´ä½“åå·®ç™¾åˆ†æ¯”ï¼ˆä¸­ä½æ•°ï¼‰: {np.median(all_overall_max_deviations):.4f}%")
        
        print(f"\n  ç¼ºå¤±é”®ç»Ÿè®¡ï¼ˆåº”è¯¥å½¢æˆé”®ä½†æœªå½¢æˆé”®çš„åŸå­å¯¹ï¼‰:")
        if len(all_missing_bond_ratios) > 0:
            print(f"    å¹³å‡ç¼ºå¤±é”®æ¯”ä¾‹: {all_missing_bond_ratios.mean():.4f} ({all_missing_bond_ratios.mean()*100:.2f}%)")
            print(f"    ä¸­ä½æ•°ç¼ºå¤±é”®æ¯”ä¾‹: {np.median(all_missing_bond_ratios):.4f} ({np.median(all_missing_bond_ratios)*100:.2f}%)")
            print(f"    ç¼ºå¤±é”®æ¯”ä¾‹èŒƒå›´: {all_missing_bond_ratios.min():.4f} - {all_missing_bond_ratios.max():.4f}")
        
        if len(all_mean_deviations) > 0:
            print(f"    ç¼ºå¤±é”®å¹³å‡åå·®ç™¾åˆ†æ¯”: {all_mean_deviations.mean():.4f}%")
            print(f"    ç¼ºå¤±é”®ä¸­ä½æ•°åå·®ç™¾åˆ†æ¯”: {np.median(all_mean_deviations):.4f}%")
            print(f"    ç¼ºå¤±é”®å¹³å‡åå·®èŒƒå›´: {all_mean_deviations.min():.4f}% - {all_mean_deviations.max():.4f}%")
        
        if len(all_max_deviations) > 0:
            print(f"    ç¼ºå¤±é”®æœ€å¤§åå·®ç™¾åˆ†æ¯”ï¼ˆå¹³å‡ï¼‰: {all_max_deviations.mean():.4f}%")
            print(f"    ç¼ºå¤±é”®æœ€å¤§åå·®ç™¾åˆ†æ¯”ï¼ˆä¸­ä½æ•°ï¼‰: {np.median(all_max_deviations):.4f}%")
            print(f"    ç¼ºå¤±é”®æœ€å¤§åå·®èŒƒå›´: {all_max_deviations.min():.4f}% - {all_max_deviations.max():.4f}%")
        
        if len(all_missing_bond_deviations) > 0:
            print(f"\n  æ‰€æœ‰ç¼ºå¤±é”®çš„åå·®åˆ†å¸ƒ:")
            print(f"    æ€»ç¼ºå¤±é”®æ•°: {len(all_missing_bond_deviations)}")
            print(f"    å¹³å‡åå·®: {all_missing_bond_deviations.mean():.4f}%")
            print(f"    ä¸­ä½æ•°åå·®: {np.median(all_missing_bond_deviations):.4f}%")
            print(f"    åå·®èŒƒå›´: {all_missing_bond_deviations.min():.4f}% - {all_missing_bond_deviations.max():.4f}%")
            print(f"    åå·®æ ‡å‡†å·®: {all_missing_bond_deviations.std():.4f}%")
        else:
            print(f"    æ³¨æ„: ä½¿ç”¨ä¸¥æ ¼æ ‡å‡†(æ ‡å‡†é”®é•¿+15pm)æœªæ£€æµ‹åˆ°ç¼ºå¤±é”®")
    else:
        print("  æ²¡æœ‰è®¡ç®—è¿ç»­æ€§æŒ‡æ ‡ï¼ˆå¯èƒ½æ‰€æœ‰åˆ†å­éƒ½å®Œå…¨è¿é€šï¼‰")
    
    print(f"\nğŸ“ æ‰€æœ‰åŸå­å¯¹è·ç¦»ç»Ÿè®¡:")
    if len(all_pair_distances) > 0:
        print(f"  å¹³å‡è·ç¦»: {all_pair_distances.mean():.4f} Ã…")
        print(f"  ä¸­ä½æ•°è·ç¦»: {np.median(all_pair_distances):.4f} Ã…")
        print(f"  æœ€å°è·ç¦»: {all_pair_distances.min():.4f} Ã…")
        print(f"  æœ€å¤§è·ç¦»: {all_pair_distances.max():.4f} Ã…")
        
        # ç»Ÿè®¡å¯èƒ½å½¢æˆé”®çš„è·ç¦»ï¼ˆ< 2.0Ã…ï¼‰
        potential_bond_threshold = 2.0
        potential_bonds = (all_pair_distances < potential_bond_threshold).sum()
        print(f"  è·ç¦» < {potential_bond_threshold}Ã… çš„åŸå­å¯¹æ•°é‡: {potential_bonds}")
        print(f"  å¯èƒ½å½¢æˆé”®çš„åŸå­å¯¹æ¯”ä¾‹: {potential_bonds / len(all_pair_distances) * 100:.2f}%")
        
        # ç»Ÿè®¡è·ç¦»è¿‡å¤§çš„åŸå­å¯¹
        large_dist_threshold = 3.0
        large_dists = (all_pair_distances > large_dist_threshold).sum()
        print(f"  è·ç¦» > {large_dist_threshold}Ã… çš„åŸå­å¯¹æ•°é‡: {large_dists}")
        print(f"  è·ç¦»è¿‡å¤§çš„åŸå­å¯¹æ¯”ä¾‹: {large_dists / len(all_pair_distances) * 100:.2f}%")
    
    print(f"\nğŸ“¦ åˆ†å­ç©ºé—´åˆ†å¸ƒç»Ÿè®¡:")
    if len(coord_ranges) > 0:
        print(f"  å¹³å‡åæ ‡èŒƒå›´ (X, Y, Z): ({coord_ranges[:, 0].mean():.2f}, {coord_ranges[:, 1].mean():.2f}, {coord_ranges[:, 2].mean():.2f}) Ã…")
        print(f"  æœ€å¤§åæ ‡èŒƒå›´: {coord_ranges.max():.2f} Ã…")
        print(f"  å¹³å‡åˆ†å­è·¨åº¦ï¼ˆæœ€å¤§åŸå­å¯¹è·ç¦»ï¼‰: {coord_spans.mean():.2f} Ã…")
        print(f"  æœ€å¤§åˆ†å­è·¨åº¦: {coord_spans.max():.2f} Ã…")
        print(f"  ä¸­ä½æ•°åˆ†å­è·¨åº¦: {np.median(coord_spans):.2f} Ã…")
        
        # ç»Ÿè®¡è·¨åº¦è¿‡å¤§çš„åˆ†å­
        large_span_threshold = 10.0  # è¶…è¿‡10Ã…è®¤ä¸ºè·¨åº¦è¿‡å¤§
        large_span_count = (coord_spans > large_span_threshold).sum()
        print(f"  è·¨åº¦ > {large_span_threshold}Ã… çš„åˆ†å­æ•°: {large_span_count} ({large_span_count/len(coord_spans)*100:.2f}%)")
        
        if len(large_gap_ratios) > 0:
            print(f"  å¹³å‡å¤§è·ç¦»åŸå­å¯¹æ¯”ä¾‹ï¼ˆ>3Ã…ï¼‰: {large_gap_ratios.mean():.2f}%")
            print(f"  ä¸­ä½æ•°å¤§è·ç¦»åŸå­å¯¹æ¯”ä¾‹: {np.median(large_gap_ratios):.2f}%")
    
    # ç”Ÿæˆå¯è§†åŒ–
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å­—ä½“
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. æœ€è¿‘è·ç¦»åˆ†å¸ƒ - ç›´æ–¹å›¾ + KDEå¯†åº¦æ›²çº¿
        if len(all_min_distances) > 0:
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # ç»˜åˆ¶ç›´æ–¹å›¾
            n, bins, patches = ax.hist(all_min_distances, bins=100, edgecolor='black', 
                                       alpha=0.7, density=False, color='steelblue', label='Histogram')
            
            # æ·»åŠ KDEå¯†åº¦æ›²çº¿
            kde = stats.gaussian_kde(all_min_distances)
            x_kde = np.linspace(all_min_distances.min(), all_min_distances.max(), 200)
            y_kde = kde(x_kde) * len(all_min_distances) * (bins[1] - bins[0])  # è½¬æ¢ä¸ºé¢‘ç‡
            ax.plot(x_kde, y_kde, 'r-', linewidth=2, label='KDE Density')
            
            # æ·»åŠ ç»Ÿè®¡çº¿
            ax.axvline(all_min_distances.mean(), color='red', linestyle='--', linewidth=2,
                      label=f'Mean: {all_min_distances.mean():.3f}Ã…')
            ax.axvline(np.median(all_min_distances), color='green', linestyle='--', linewidth=2,
                      label=f'Median: {np.median(all_min_distances):.3f}Ã…')
            
            ax.set_xlabel('Nearest Atom Distance (Ã…)', fontsize=12)
            ax.set_ylabel('Frequency', fontsize=12)
            ax.set_title('Nearest Atom Distance Distribution', fontsize=13, fontweight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            fig_path = output_dir / "nearest_atom_distance_distribution.png"
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            print(f"æœ€è¿‘åŸå­è·ç¦»åˆ†å¸ƒç»Ÿè®¡å›¾å·²ä¿å­˜åˆ°: {fig_path}")
            plt.close()
        
        # 2. ç»¼åˆç»“æ„åˆ†æå›¾ï¼ˆåŒ…å«æœ€è¿‘è·ç¦»ã€é”®é•¿ã€è¿é€šæ€§ç­‰ï¼‰
        if len(all_min_distances) > 0:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # æœ€è¿‘è·ç¦»ç›´æ–¹å›¾
            axes[0, 0].hist(all_min_distances, bins=100, edgecolor='black', alpha=0.7)
            axes[0, 0].axvline(all_min_distances.mean(), color='r', linestyle='--', 
                              label=f'Mean: {all_min_distances.mean():.3f}Ã…')
            axes[0, 0].axvline(np.median(all_min_distances), color='g', linestyle='--', 
                              label=f'Median: {np.median(all_min_distances):.3f}Ã…')
            axes[0, 0].set_xlabel('Nearest Atom Distance (Ã…)')
            axes[0, 0].set_ylabel('Frequency')
            axes[0, 0].set_title('Nearest Atom Distance Distribution')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # æœ€è¿‘è·ç¦»ç´¯ç§¯åˆ†å¸ƒ
            sorted_dists = np.sort(all_min_distances)
            cumulative = np.arange(1, len(sorted_dists) + 1) / len(sorted_dists)
            axes[0, 1].plot(sorted_dists, cumulative, linewidth=2)
            axes[0, 1].axvline(3.0, color='r', linestyle='--', label='3.0Ã… threshold')
            axes[0, 1].set_xlabel('Nearest Atom Distance (Ã…)')
            axes[0, 1].set_ylabel('Cumulative Probability')
            axes[0, 1].set_title('Cumulative Distribution of Nearest Distances')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # é”®é•¿åˆ†å¸ƒ
            if len(bond_lengths) > 0:
                axes[1, 0].hist(bond_lengths, bins=100, edgecolor='black', alpha=0.7, color='orange')
                axes[1, 0].axvline(bond_lengths.mean(), color='r', linestyle='--', 
                                  label=f'Mean: {bond_lengths.mean():.3f}Ã…')
                axes[1, 0].axvline(np.median(bond_lengths), color='g', linestyle='--', 
                                  label=f'Median: {np.median(bond_lengths):.3f}Ã…')
                axes[1, 0].set_xlabel('Bond Length (Ã…)')
                axes[1, 0].set_ylabel('Frequency')
                axes[1, 0].set_title('Bond Length Distribution')
                axes[1, 0].legend()
                axes[1, 0].grid(True, alpha=0.3)
            else:
                axes[1, 0].text(0.5, 0.5, 'No bonds detected', 
                              ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Bond Length Distribution (No Data)')
            
            # è¿é€šåˆ†é‡æ•°åˆ†å¸ƒ
            unique_components, counts = np.unique(num_components_list, return_counts=True)
            axes[1, 1].bar(unique_components, counts, edgecolor='black', alpha=0.7, color='green')
            axes[1, 1].set_xlabel('Number of Connected Components')
            axes[1, 1].set_ylabel('Number of Molecules')
            axes[1, 1].set_title('Connected Components Distribution')
            axes[1, 1].grid(True, alpha=0.3, axis='y')
            
            plt.tight_layout()
            fig_path = output_dir / "molecule_structure_analysis.png"
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            print(f"ç»¼åˆç»“æ„åˆ†æå›¾å·²ä¿å­˜åˆ°: {fig_path}")
            plt.close()
        
        # 3. åˆ†å­è·¨åº¦åˆ†å¸ƒ
        if len(coord_spans) > 0:
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.hist(coord_spans, bins=50, edgecolor='black', alpha=0.7, color='red')
            ax.axvline(coord_spans.mean(), color='r', linestyle='--', 
                      label=f'Mean: {coord_spans.mean():.2f}Ã…')
            ax.axvline(np.median(coord_spans), color='g', linestyle='--', 
                      label=f'Median: {np.median(coord_spans):.2f}Ã…')
            ax.axvline(10.0, color='orange', linestyle='--', label='10.0Ã… threshold')
            ax.set_xlabel('Molecular Span (Max Atom Pair Distance, Ã…)')
            ax.set_ylabel('Number of Molecules')
            ax.set_title('Molecular Span Distribution (Reflecting Atom Dispersion)')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            fig_path = output_dir / "molecule_span_distribution.png"
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            print(f"åˆ†å­è·¨åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {fig_path}")
            plt.close()
        
        # 2. æ‰€æœ‰åŸå­å¯¹è·ç¦»åˆ†å¸ƒï¼ˆå¦‚æœæ•°æ®é‡ä¸å¤ªå¤§ï¼‰
        if len(all_pair_distances) > 0 and len(all_pair_distances) < 1000000:
            fig, ax = plt.subplots(figsize=(10, 6))
            # åªæ˜¾ç¤ºåˆç†èŒƒå›´çš„è·ç¦»
            valid_dists = all_pair_distances[all_pair_distances < 10.0]
            ax.hist(valid_dists, bins=200, edgecolor='black', alpha=0.7, color='purple')
            ax.axvline(valid_dists.mean(), color='r', linestyle='--', 
                      label=f'Mean: {valid_dists.mean():.3f}Ã…')
            ax.axvline(2.0, color='orange', linestyle='--', label='2.0Ã… (potential bond)')
            ax.set_xlabel('Atom Pair Distance (Ã…)')
            ax.set_ylabel('Frequency')
            ax.set_title('Atom Pair Distance Distribution (< 10Ã…)')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            fig_path = output_dir / "pairwise_distance_distribution.png"
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            print(f"åŸå­å¯¹è·ç¦»åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {fig_path}")
            plt.close()
        
        # 6. è¿ç»­æ€§æŒ‡æ ‡å¯è§†åŒ–
        if len(all_continuity_scores) > 0:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # è¿ç»­æ€§åˆ†æ•°åˆ†å¸ƒ
            axes[0, 0].hist(all_continuity_scores, bins=50, edgecolor='black', alpha=0.7, color='purple')
            axes[0, 0].axvline(all_continuity_scores.mean(), color='r', linestyle='--', 
                              label=f'Mean: {all_continuity_scores.mean():.3f}')
            axes[0, 0].axvline(np.median(all_continuity_scores), color='g', linestyle='--', 
                              label=f'Median: {np.median(all_continuity_scores):.3f}')
            axes[0, 0].set_xlabel('Connectivity Continuity Score', fontsize=12)
            axes[0, 0].set_ylabel('Number of Molecules', fontsize=12)
            axes[0, 0].set_title('Connectivity Continuity Score Distribution', fontsize=13, fontweight='bold')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # ç¼ºå¤±é”®åå·®åˆ†å¸ƒ
            if len(all_missing_bond_deviations) > 0:
                axes[0, 1].hist(all_missing_bond_deviations, bins=50, edgecolor='black', alpha=0.7, color='orange')
                axes[0, 1].axvline(all_missing_bond_deviations.mean(), color='r', linestyle='--', 
                                  label=f'Mean: {all_missing_bond_deviations.mean():.2f}%')
                axes[0, 1].axvline(np.median(all_missing_bond_deviations), color='g', linestyle='--', 
                                  label=f'Median: {np.median(all_missing_bond_deviations):.2f}%')
                axes[0, 1].set_xlabel('Missing Bond Deviation (%)', fontsize=12)
                axes[0, 1].set_ylabel('Frequency', fontsize=12)
                axes[0, 1].set_title('Missing Bond Deviation Distribution', fontsize=13, fontweight='bold')
                axes[0, 1].legend()
                axes[0, 1].grid(True, alpha=0.3)
            else:
                axes[0, 1].text(0.5, 0.5, 'No missing bonds detected', 
                              ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Missing Bond Deviation Distribution (No Data)')
            
            # ç¼ºå¤±é”®æ¯”ä¾‹åˆ†å¸ƒ
            if len(all_missing_bond_ratios) > 0:
                axes[1, 0].hist(all_missing_bond_ratios * 100, bins=50, edgecolor='black', alpha=0.7, color='cyan')
                axes[1, 0].axvline(all_missing_bond_ratios.mean() * 100, color='r', linestyle='--', 
                                  label=f'Mean: {all_missing_bond_ratios.mean()*100:.2f}%')
                axes[1, 0].axvline(np.median(all_missing_bond_ratios) * 100, color='g', linestyle='--', 
                                  label=f'Median: {np.median(all_missing_bond_ratios)*100:.2f}%')
                axes[1, 0].set_xlabel('Missing Bond Ratio (%)', fontsize=12)
                axes[1, 0].set_ylabel('Number of Molecules', fontsize=12)
                axes[1, 0].set_title('Missing Bond Ratio Distribution', fontsize=13, fontweight='bold')
                axes[1, 0].legend()
                axes[1, 0].grid(True, alpha=0.3)
            else:
                axes[1, 0].text(0.5, 0.5, 'No data', 
                              ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Missing Bond Ratio Distribution (No Data)')
            
            # è¿ç»­æ€§åˆ†æ•° vs ç¢ç‰‡æ•°æ•£ç‚¹å›¾
            if len(num_components_list) == len(all_continuity_scores):
                scatter = axes[1, 1].scatter(num_components_list, all_continuity_scores, 
                                            alpha=0.6, s=50, c=all_continuity_scores, 
                                            cmap='viridis', edgecolors='black', linewidths=0.5)
                axes[1, 1].set_xlabel('Number of Connected Components', fontsize=12)
                axes[1, 1].set_ylabel('Connectivity Continuity Score', fontsize=12)
                axes[1, 1].set_title('Continuity Score vs Number of Components', fontsize=13, fontweight='bold')
                axes[1, 1].grid(True, alpha=0.3)
                plt.colorbar(scatter, ax=axes[1, 1], label='Continuity Score')
            else:
                axes[1, 1].text(0.5, 0.5, 'Data length mismatch', 
                              ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Continuity Score vs Number of Components (No Data)')
            
            plt.tight_layout()
            fig_path = output_dir / "connectivity_continuity_analysis.png"
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            print(f"è¿ç»­æ€§è¿é€šæ€§åˆ†æå›¾å·²ä¿å­˜åˆ°: {fig_path}")
            plt.close()
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶
        results_file = output_dir / "structure_analysis_results.txt"
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write("åˆ†å­ç»“æ„åˆ†æç»“æœ\n")
            f.write("="*60 + "\n\n")
            f.write(f"æ€»åˆ†å­æ•°: {len(npz_files)}\n")
            f.write(f"å¹³å‡åŸå­æ•°: {num_atoms_list.mean():.2f}\n\n")
            
            if len(all_min_distances) > 0:
                f.write("æœ€è¿‘åŸå­è·ç¦»ç»Ÿè®¡:\n")
                f.write(f"  å¹³å‡: {all_min_distances.mean():.4f} Ã…\n")
                f.write(f"  ä¸­ä½æ•°: {np.median(all_min_distances):.4f} Ã…\n")
                f.write(f"  èŒƒå›´: {all_min_distances.min():.4f} - {all_min_distances.max():.4f} Ã…\n")
                f.write(f"  æ ‡å‡†å·®: {all_min_distances.std():.4f} Ã…\n\n")
            
            if len(bond_lengths) > 0:
                f.write("é”®é•¿ç»Ÿè®¡:\n")
                f.write(f"  æ€»é”®æ•°: {len(bond_lengths)}\n")
                f.write(f"  å¹³å‡: {bond_lengths.mean():.4f} Ã…\n")
                f.write(f"  èŒƒå›´: {bond_lengths.min():.4f} - {bond_lengths.max():.4f} Ã…\n\n")
            
            f.write("è¿é€šæ€§ç»Ÿè®¡:\n")
            f.write(f"  è¿é€šåˆ†å­æ¯”ä¾‹: {is_connected_list.sum() / len(is_connected_list) * 100:.2f}%\n")
            f.write(f"  å¹³å‡è¿é€šåˆ†é‡æ•°: {num_components_list.mean():.2f}\n\n")
            
            if len(all_continuity_scores) > 0:
                f.write("è¿ç»­æ€§è¿é€šæ€§æŒ‡æ ‡ (ä½¿ç”¨ margin1={}pm):\n".format(margin1))
                f.write(f"  å¹³å‡è¿ç»­æ€§åˆ†æ•°: {all_continuity_scores.mean():.4f}\n")
                f.write(f"  ä¸­ä½æ•°è¿ç»­æ€§åˆ†æ•°: {np.median(all_continuity_scores):.4f}\n")
                f.write(f"  è¿ç»­æ€§åˆ†æ•°èŒƒå›´: {all_continuity_scores.min():.4f} - {all_continuity_scores.max():.4f}\n")
                if len(all_missing_bond_ratios) > 0:
                    f.write(f"  å¹³å‡ç¼ºå¤±é”®æ¯”ä¾‹: {all_missing_bond_ratios.mean():.4f} ({all_missing_bond_ratios.mean()*100:.2f}%)\n")
                if len(all_mean_deviations) > 0:
                    f.write(f"  å¹³å‡åå·®ç™¾åˆ†æ¯”: {all_mean_deviations.mean():.4f}%\n")
                if len(all_missing_bond_deviations) > 0:
                    f.write(f"  æ€»ç¼ºå¤±é”®æ•°: {len(all_missing_bond_deviations)}\n")
                    f.write(f"  ç¼ºå¤±é”®å¹³å‡åå·®: {all_missing_bond_deviations.mean():.4f}%\n")
        
        print(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return {
        'min_distances': all_min_distances,
        'pair_distances': all_pair_distances,
        'bond_lengths': bond_lengths,
        'num_components': num_components_list,
        'is_connected': is_connected_list,
        'num_atoms': num_atoms_list,
        'continuity_scores': all_continuity_scores,
        'missing_bond_ratios': all_missing_bond_ratios,
        'mean_deviations': all_mean_deviations,
        'max_deviations': all_max_deviations,
        'missing_bond_deviations': all_missing_bond_deviations
    }


def main():
    parser = argparse.ArgumentParser(description='åˆ†æç”Ÿæˆåˆ†å­çš„ç»“æ„é—®é¢˜')
    parser.add_argument(
        '--molecule_dir',
        type=str,
        required=True,
        help='åŒ…å« .npz åˆ†å­æ–‡ä»¶çš„ç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰'
    )
    
    args = parser.parse_args()
    
    print("="*60)
    print("åˆ†å­ç»“æ„åˆ†æå·¥å…·")
    print("="*60)
    print(f"åˆ†å­ç›®å½•: {args.molecule_dir}")
    print("="*60)
    
    results = analyze_molecules(args.molecule_dir, args.output_dir)
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()

