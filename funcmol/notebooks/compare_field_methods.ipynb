{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a52041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50dda8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import hydra\n",
    "from pathlib import Path\n",
    "from lightning import Fabric\n",
    "\n",
    "# 设置 torch.compile 兼容性\n",
    "try:\n",
    "    import torch._dynamo\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "except ImportError:\n",
    "    # PyTorch 版本 < 2.0 不支持 torch._dynamo\n",
    "    print(\"Warning: torch._dynamo not available in this PyTorch version\")\n",
    "\n",
    "## set up environment\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from funcmol.utils.constants import PADDING_INDEX\n",
    "from funcmol.utils.gnf_visualizer import (\n",
    "    load_config_from_exp_dir, load_model, \n",
    "    create_converter, GNFVisualizer, create_gnf_converter\n",
    ")\n",
    "from funcmol.dataset.dataset_field import create_field_loaders\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "# 模型根目录\n",
    "model_root = \"/datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/neural_field\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3cdc716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option: gt_pred\n",
      "Model directory: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9_20250804_153549_358664\n",
      "Output directory: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9_20250804_153549_358664\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "exp_name = 'nf_qm9_20250804_153549_358664'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 判断是gt_only还是gt_pred模式\n",
    "if '2025' in exp_name:  # gt + predicted field\n",
    "    option = 'gt_pred'\n",
    "    model_dir = os.path.join(model_root, exp_name)\n",
    "else:  # gt only. exp_name is the name of field (e.g., gaussian_mag)\n",
    "    option = 'gt_only'\n",
    "    model_dir = os.path.join(model_root, exp_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "output_dir = model_dir\n",
    "print(f\"Option: {option}\")\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7476f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/funcmol/dataset/data\n",
      "数据集总大小: 20042\n",
      "从 20042 个样本中随机选择 1000 个\n",
      "开始加载样本...\n",
      "已加载 0/1000 个样本...\n",
      "已加载 100/1000 个样本...\n",
      "已加载 200/1000 个样本...\n",
      "已加载 300/1000 个样本...\n",
      "已加载 400/1000 个样本...\n",
      "已加载 500/1000 个样本...\n",
      "已加载 600/1000 个样本...\n",
      "已加载 700/1000 个样本...\n",
      "已加载 800/1000 个样本...\n",
      "已加载 900/1000 个样本...\n",
      "成功加载 1000 个样本\n",
      "数据形状: coords=torch.Size([1000, 18, 3]), types=torch.Size([1000, 18])\n",
      "Data loaded: torch.Size([1000, 18, 3]), torch.Size([1000, 18])\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "fabric = Fabric(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    precision=\"32-true\",\n",
    "    strategy=\"auto\"\n",
    ")\n",
    "fabric.launch()\n",
    "\n",
    "# 从实验目录加载配置\n",
    "config = load_config_from_exp_dir(model_dir)\n",
    "\n",
    "# 准备数据 - 直接从数据集加载，确保数据代表性\n",
    "def prepare_data_directly(config, device, max_samples=1000):\n",
    "    \"\"\"直接从数据集加载数据，绕过DataLoader的限制\"\"\"\n",
    "    from funcmol.dataset.dataset_field import FieldDataset\n",
    "    \n",
    "    # 创建GNF转换器\n",
    "    gnf_converter = create_gnf_converter(config, device=\"cpu\")\n",
    "    \n",
    "    # 直接创建数据集实例，不使用DataLoader\n",
    "    dataset = FieldDataset(\n",
    "        gnf_converter=gnf_converter,\n",
    "        dset_name=config[\"dset\"][\"dset_name\"],\n",
    "        data_dir=config[\"dset\"][\"data_dir\"],\n",
    "        elements=config[\"dset\"][\"elements\"],\n",
    "        split=\"val\",\n",
    "        n_points=config[\"dset\"][\"n_points\"],\n",
    "        rotate=False,  # 验证时不旋转\n",
    "        resolution=config[\"dset\"][\"resolution\"],\n",
    "        grid_dim=config[\"dset\"][\"grid_dim\"],\n",
    "        radius=config[\"dset\"][\"atomic_radius\"],\n",
    "        sample_full_grid=False,\n",
    "        debug_one_mol=False,\n",
    "        debug_subset=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"数据集总大小: {len(dataset)}\")\n",
    "    \n",
    "    # 随机采样指定数量的样本\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    if max_samples >= len(dataset):\n",
    "        sample_indices = list(range(len(dataset)))\n",
    "        print(f\"请求样本数({max_samples}) >= 数据集大小({len(dataset)})，使用所有样本\")\n",
    "    else:\n",
    "        sample_indices = random.sample(range(len(dataset)), max_samples)\n",
    "        print(f\"从 {len(dataset)} 个样本中随机选择 {max_samples} 个\")\n",
    "    \n",
    "    # 加载选中的样本\n",
    "    all_coords = []\n",
    "    all_types = []\n",
    "    \n",
    "    print(\"开始加载样本...\")\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"已加载 {i}/{len(sample_indices)} 个样本...\")\n",
    "        \n",
    "        # 直接从数据集获取样本\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # 提取坐标和原子类型\n",
    "        coords = sample.pos  # [n_atoms, 3]\n",
    "        atoms_channel = sample.x  # [n_atoms]\n",
    "        \n",
    "        # 移除padding\n",
    "        valid_mask = atoms_channel != PADDING_INDEX\n",
    "        coords = coords[valid_mask]\n",
    "        atoms_channel = atoms_channel[valid_mask]\n",
    "        \n",
    "        # 填充到固定长度（与原始数据格式一致）\n",
    "        max_atoms = 18  # 根据原始数据设置\n",
    "        if len(coords) < max_atoms:\n",
    "            # 填充到固定长度\n",
    "            pad_coords = torch.zeros(max_atoms - len(coords), 3)\n",
    "            pad_atoms = torch.full((max_atoms - len(atoms_channel),), PADDING_INDEX, dtype=atoms_channel.dtype)\n",
    "            \n",
    "            coords = torch.cat([coords, pad_coords], dim=0)\n",
    "            atoms_channel = torch.cat([atoms_channel, pad_atoms], dim=0)\n",
    "        elif len(coords) > max_atoms:\n",
    "            # 截断到固定长度\n",
    "            coords = coords[:max_atoms]\n",
    "            atoms_channel = atoms_channel[:max_atoms]\n",
    "        \n",
    "        all_coords.append(coords.unsqueeze(0))  # [1, max_atoms, 3]\n",
    "        all_types.append(atoms_channel.unsqueeze(0))  # [1, max_atoms]\n",
    "    \n",
    "    # 合并所有样本\n",
    "    gt_coords = torch.cat(all_coords, dim=0)  # [n_samples, max_atoms, 3]\n",
    "    gt_types = torch.cat(all_types, dim=0)    # [n_samples, max_atoms]\n",
    "    \n",
    "    print(f\"成功加载 {len(gt_coords)} 个样本\")\n",
    "    print(f\"数据形状: coords={gt_coords.shape}, types={gt_types.shape}\")\n",
    "    \n",
    "    # 创建一个示例batch用于兼容性\n",
    "    sample_batch = dataset[0]\n",
    "    \n",
    "    return sample_batch, gt_coords, gt_types\n",
    "\n",
    "# 加载数据\n",
    "batch, gt_coords, gt_types = prepare_data_directly_from_dataset(config, device, max_samples=1000)\n",
    "print(f\"Data loaded: {gt_coords.shape}, {gt_types.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a83c15a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model from: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9_20250804_153549_358664\n",
      "GT field comparison mode - no encoder/decoder needed\n",
      "GNF Converter created with n_iter: 2000, gradient_field_method: tanh, n_atom_types: 5\n",
      "GNF Converter created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessing model from: {model_dir}\")\n",
    "\n",
    "## Load model (for GT field comparison, we don't need encoder/decoder)\n",
    "encoder, decoder = None, None\n",
    "codes = None\n",
    "print(\"GT field comparison mode - no encoder/decoder needed\")\n",
    "\n",
    "# 创建转换器用于GT field计算\n",
    "converter = create_converter(config, device)\n",
    "print(f\"GNF Converter created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38160495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSD计算函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 添加RMSD计算函数（双向匹配）\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def compute_rmsd_hungarian(coords1, coords2):\n",
    "    \"\"\"\n",
    "    计算两个分子坐标集合之间的对称RMSD，使用Hungarian算法进行最优匹配\n",
    "    \n",
    "    Args:\n",
    "        coords1: 第一个分子坐标 [n_atoms1, 3]\n",
    "        coords2: 第二个分子坐标 [n_atoms2, 3]\n",
    "    \n",
    "    Returns:\n",
    "        rmsd: 对称RMSD值\n",
    "    \"\"\"\n",
    "    coords1 = coords1.detach().cpu().numpy() if torch.is_tensor(coords1) else coords1\n",
    "    coords2 = coords2.detach().cpu().numpy() if torch.is_tensor(coords2) else coords2\n",
    "    \n",
    "    n1, n2 = len(coords1), len(coords2)\n",
    "    \n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # 计算距离矩阵\n",
    "    dist_matrix = np.sqrt(np.sum((coords1[:, np.newaxis, :] - coords2[np.newaxis, :, :])**2, axis=2))\n",
    "    \n",
    "    # 使用Hungarian算法找到最优匹配\n",
    "    if n1 <= n2:\n",
    "        # coords1中的每个点匹配到coords2中的点\n",
    "        row_indices, col_indices = linear_sum_assignment(dist_matrix)\n",
    "        matched_distances = dist_matrix[row_indices, col_indices]\n",
    "        \n",
    "        # 计算双向RMSD\n",
    "        # 方向1: coords1 -> coords2\n",
    "        rmsd_1to2 = np.sqrt(np.mean(matched_distances**2))\n",
    "        \n",
    "        # 方向2: coords2 -> coords1 (未匹配的点到最近点的距离)\n",
    "        matched_cols = set(col_indices)\n",
    "        unmatched_cols = [i for i in range(n2) if i not in matched_cols]\n",
    "        \n",
    "        if unmatched_cols:\n",
    "            unmatched_coords2 = coords2[unmatched_cols]\n",
    "            min_dists_2to1 = []\n",
    "            for coord in unmatched_coords2:\n",
    "                dists_to_coords1 = np.sqrt(np.sum((coords1 - coord)**2, axis=1))\n",
    "                min_dists_2to1.append(np.min(dists_to_coords1))\n",
    "            min_dists_2to1 = np.array(min_dists_2to1)  # 转换为numpy数组\n",
    "            rmsd_2to1 = np.sqrt(np.mean(min_dists_2to1**2))\n",
    "        else:\n",
    "            rmsd_2to1 = 0.0\n",
    "        \n",
    "        # 对称RMSD\n",
    "        symmetric_rmsd = (rmsd_1to2 + rmsd_2to1) / 2\n",
    "        \n",
    "    else:\n",
    "        # coords2中的每个点匹配到coords1中的点\n",
    "        row_indices, col_indices = linear_sum_assignment(dist_matrix.T)\n",
    "        # 注意：当使用dist_matrix.T时，row_indices对应coords2，col_indices对应coords1\n",
    "        matched_distances = dist_matrix[col_indices, row_indices]  # 修正索引顺序\n",
    "        \n",
    "        # 计算双向RMSD\n",
    "        # 方向1: coords2 -> coords1\n",
    "        rmsd_2to1 = np.sqrt(np.mean(matched_distances**2))\n",
    "        \n",
    "        # 方向2: coords1 -> coords2 (未匹配的点到最近点的距离)\n",
    "        matched_rows = set(row_indices)\n",
    "        unmatched_rows = [i for i in range(n1) if i not in matched_rows]\n",
    "        \n",
    "        if unmatched_rows:\n",
    "            unmatched_coords1 = coords1[unmatched_rows]\n",
    "            min_dists_1to2 = []\n",
    "            for coord in unmatched_coords1:\n",
    "                dists_to_coords2 = np.sqrt(np.sum((coords2 - coord)**2, axis=1))\n",
    "                min_dists_1to2.append(np.min(dists_to_coords2))\n",
    "            min_dists_1to2 = np.array(min_dists_1to2)  # 转换为numpy数组\n",
    "            rmsd_1to2 = np.sqrt(np.mean(min_dists_1to2**2))\n",
    "        else:\n",
    "            rmsd_1to2 = 0.0\n",
    "        \n",
    "        # 对称RMSD\n",
    "        symmetric_rmsd = (rmsd_1to2 + rmsd_2to1) / 2\n",
    "    \n",
    "    return symmetric_rmsd\n",
    "\n",
    "def reconstruction_loss(coords1, points):\n",
    "    \"\"\"Calculate reconstruction loss between original coordinates and sampled points\"\"\"\n",
    "    # Calculate pairwise distances between all points\n",
    "    dist1 = torch.sum((coords1.unsqueeze(1) - points.unsqueeze(0))**2, dim=2)\n",
    "    \n",
    "    eps = 1e-8\n",
    "    \n",
    "    # For each original atom, find the closest sampled point\n",
    "    min_dist_to_samples = torch.min(dist1 + eps, dim=1)[0]\n",
    "    \n",
    "    # For each sampled point, find the closest original atom\n",
    "    min_dist_to_atoms = torch.min(dist1 + eps, dim=0)[0]\n",
    "    \n",
    "    # Combine both directions\n",
    "    coverage_loss = torch.mean(min_dist_to_samples)  # 确保每个原子都有近邻采样点\n",
    "    clustering_loss = torch.mean(min_dist_to_atoms)  # 确保采样点集中在原子位置附近\n",
    "    \n",
    "    # 总损失是两个方向的加权和\n",
    "    total_loss = coverage_loss + 0.1 * clustering_loss  # 可以调整权重\n",
    "    \n",
    "    return torch.sqrt(total_loss)\n",
    "\n",
    "print(\"RMSD计算函数已定义\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "153becfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Field函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# 定义gt field函数\n",
    "def create_gt_field_func(converter, gt_coords, gt_types, sample_idx):\n",
    "    \"\"\"创建真实梯度场函数\"\"\"\n",
    "    def gt_field_func(points):\n",
    "        gt_mask = (gt_types[sample_idx] != PADDING_INDEX)\n",
    "        gt_valid_coords = gt_coords[sample_idx][gt_mask]\n",
    "        gt_valid_types = gt_types[sample_idx][gt_mask]\n",
    "        \n",
    "        # 确保 points 是正确的形状\n",
    "        if points.dim() == 2:  # [n_points, 3]\n",
    "            points = points.unsqueeze(0)  # [1, n_points, 3]\n",
    "        elif points.dim() == 3:  # [batch, n_points, 3]\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected points shape: {points.shape}\")\n",
    "        \n",
    "        result = converter.mol2gnf(\n",
    "            gt_valid_coords.unsqueeze(0),\n",
    "            gt_valid_types.unsqueeze(0),\n",
    "            points\n",
    "        )\n",
    "        # 确保返回 [n_points, n_atom_types, 3] 形状\n",
    "        if result.dim() == 4:  # [batch, n_points, n_atom_types, 3]\n",
    "            return result[0]  # 取第一个batch\n",
    "        else:\n",
    "            return result\n",
    "    return gt_field_func\n",
    "\n",
    "print(\"GT Field函数定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c3d0f6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际数据集大小: 1000\n",
      "将分析 100 个样本\n",
      "样本索引范围: 6 - 990\n",
      "前10个样本: [6, 25, 27, 30, 32, 44, 46, 71, 80, 81]\n",
      "后10个样本: [890, 906, 913, 947, 954, 964, 978, 983, 986, 990]\n",
      "数据集大小: 1000\n",
      "请求样本数: 100\n"
     ]
    }
   ],
   "source": [
    "# 设置参数 - 批量分析300个样本\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 批量分析参数\n",
    "TOTAL_SAMPLES = 100  # 要分析的样本总数（可选）\n",
    "BATCH_SIZE = 20      # 每批处理的样本数（可选）\n",
    "RANDOM_SEED = 42     # 随机种子，确保结果可重现\n",
    "\n",
    "# 设置随机种子\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 获取实际数据集大小\n",
    "actual_dataset_size = len(gt_coords)\n",
    "print(f\"实际数据集大小: {actual_dataset_size}\")\n",
    "\n",
    "# 根据实际数据集大小调整采样策略\n",
    "if TOTAL_SAMPLES > actual_dataset_size:\n",
    "    print(f\"警告: 请求的样本数({TOTAL_SAMPLES})大于数据集大小({actual_dataset_size})\")\n",
    "    print(f\"将使用所有可用样本: {actual_dataset_size}\")\n",
    "    TOTAL_SAMPLES = actual_dataset_size\n",
    "\n",
    "# 方法1：随机采样\n",
    "sample_indices = random.sample(range(actual_dataset_size), TOTAL_SAMPLES)\n",
    "\n",
    "# 方法2：连续采样（如果你想分析前N个样本）\n",
    "# sample_indices = list(range(TOTAL_SAMPLES))\n",
    "\n",
    "# 方法3：分层采样（如果你想均匀分布在不同区间）\n",
    "# def stratified_sampling(total_samples, n_samples):\n",
    "#     intervals = np.linspace(0, total_samples, 10)\n",
    "#     samples_per_interval = n_samples // 10\n",
    "#     sample_indices = []\n",
    "#     for i in range(len(intervals)-1):\n",
    "#         start = int(intervals[i])\n",
    "#         end = int(intervals[i+1])\n",
    "#         interval_samples = random.sample(range(start, end), samples_per_interval)\n",
    "#         sample_indices.extend(interval_samples)\n",
    "#     return sample_indices\n",
    "# sample_indices = stratified_sampling(actual_dataset_size, TOTAL_SAMPLES)\n",
    "\n",
    "# 排序以便于跟踪\n",
    "sample_indices.sort()\n",
    "\n",
    "field_methods = ['gaussian_mag', 'tanh']  # 要比较的field方法\n",
    "output_dir = \"/datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/funcmol/field_set\"\n",
    "\n",
    "print(f\"将分析 {len(sample_indices)} 个样本\")\n",
    "print(f\"样本索引范围: {min(sample_indices)} - {max(sample_indices)}\")\n",
    "print(f\"前10个样本: {sample_indices[:10]}\")\n",
    "print(f\"后10个样本: {sample_indices[-10:]}\")\n",
    "print(f\"数据集大小: {actual_dataset_size}\")\n",
    "print(f\"请求样本数: {TOTAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出目录: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/funcmol/field_set\n",
      "\n",
      "创建 gaussian_mag gt field转换器...\n",
      "Dataset directory: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/funcmol/dataset/data\n",
      "GNF Converter created with n_iter: 2000, gradient_field_method: gaussian_mag, n_atom_types: 5\n",
      "gaussian_mag gt field转换器创建完成\n",
      "\n",
      "创建 tanh gt field转换器...\n",
      "Dataset directory: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/funcmol/dataset/data\n",
      "GNF Converter created with n_iter: 2000, gradient_field_method: tanh, n_atom_types: 5\n",
      "tanh gt field转换器创建完成\n",
      "\n",
      "开始gt field比较...\n",
      "启用分批处理：将分 5 批处理\n",
      "\n",
      "处理第 1 批，样本 0 到 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 6\n",
      "Ground truth atoms: 14\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1880\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1840\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1962\n",
      "[DBSCAN] Total points: 2000, Clusters found: 3, Noise points: 1911\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 6\n",
      "Ground truth atoms: 14\n",
      "[DBSCAN] Total points: 1000, Clusters found: 7, Noise points: 432\n",
      "[DBSCAN] Total points: 1000, Clusters found: 5, Noise points: 297\n",
      "[DBSCAN] Total points: 1000, Clusters found: 1, Noise points: 751\n",
      "[DBSCAN] Total points: 1000, Clusters found: 5, Noise points: 467\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:   5%|▌         | 1/20 [01:42<32:28, 102.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 25\n",
      "Ground truth atoms: 18\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1928\n",
      "[DBSCAN] Total points: 2000, Clusters found: 9, Noise points: 1862\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1981\n",
      "[DBSCAN] Total points: 2000, Clusters found: 3, Noise points: 1954\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 25\n",
      "Ground truth atoms: 18\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 832\n",
      "[DBSCAN] Total points: 1000, Clusters found: 10, Noise points: 608\n",
      "[DBSCAN] Total points: 1000, Clusters found: 2, Noise points: 903\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 821\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  10%|█         | 2/20 [03:46<34:34, 115.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 27\n",
      "Ground truth atoms: 18\n",
      "[DBSCAN] Total points: 2000, Clusters found: 4, Noise points: 1950\n",
      "[DBSCAN] Total points: 2000, Clusters found: 9, Noise points: 1868\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1986\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 27\n",
      "Ground truth atoms: 18\n",
      "[DBSCAN] Total points: 1000, Clusters found: 5, Noise points: 905\n",
      "[DBSCAN] Total points: 1000, Clusters found: 11, Noise points: 736\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n",
      "[DBSCAN] Total points: 1000, Clusters found: 2, Noise points: 939\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  15%|█▌        | 3/20 [05:47<33:20, 117.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 30\n",
      "Ground truth atoms: 17\n",
      "[DBSCAN] Total points: 2000, Clusters found: 6, Noise points: 1877\n",
      "[DBSCAN] Total points: 2000, Clusters found: 8, Noise points: 1816\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1980\n",
      "[DBSCAN] Total points: 2000, Clusters found: 2, Noise points: 1952\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 30\n",
      "Ground truth atoms: 17\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 641\n",
      "[DBSCAN] Total points: 1000, Clusters found: 8, Noise points: 394\n",
      "[DBSCAN] Total points: 1000, Clusters found: 2, Noise points: 805\n",
      "[DBSCAN] Total points: 1000, Clusters found: 3, Noise points: 761\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  20%|██        | 4/20 [12:10<59:17, 222.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 32\n",
      "Ground truth atoms: 16\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1937\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1937\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1981\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1988\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 32\n",
      "Ground truth atoms: 16\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 840\n",
      "[DBSCAN] Total points: 1000, Clusters found: 9, Noise points: 745\n",
      "[DBSCAN] Total points: 1000, Clusters found: 2, Noise points: 921\n",
      "[DBSCAN] Total points: 1000, Clusters found: 1, Noise points: 980\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  25%|██▌       | 5/20 [14:06<46:01, 184.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 44\n",
      "Ground truth atoms: 18\n",
      "[DBSCAN] Total points: 2000, Clusters found: 8, Noise points: 1869\n",
      "[DBSCAN] Total points: 2000, Clusters found: 8, Noise points: 1859\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1984\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 44\n",
      "Ground truth atoms: 18\n",
      "[DBSCAN] Total points: 1000, Clusters found: 10, Noise points: 653\n",
      "[DBSCAN] Total points: 1000, Clusters found: 10, Noise points: 562\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n",
      "[DBSCAN] Total points: 1000, Clusters found: 3, Noise points: 889\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  30%|███       | 6/20 [16:03<37:38, 161.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 46\n",
      "Ground truth atoms: 17\n",
      "[DBSCAN] Total points: 2000, Clusters found: 7, Noise points: 1851\n",
      "[DBSCAN] Total points: 2000, Clusters found: 8, Noise points: 1828\n",
      "[DBSCAN] Total points: 2000, Clusters found: 2, Noise points: 1955\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 46\n",
      "Ground truth atoms: 17\n",
      "[DBSCAN] Total points: 1000, Clusters found: 8, Noise points: 699\n",
      "[DBSCAN] Total points: 1000, Clusters found: 12, Noise points: 525\n",
      "[DBSCAN] Total points: 1000, Clusters found: 3, Noise points: 721\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  35%|███▌      | 7/20 [22:06<49:12, 227.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 71\n",
      "Ground truth atoms: 13\n",
      "[DBSCAN] Total points: 2000, Clusters found: 4, Noise points: 1734\n",
      "[DBSCAN] Total points: 2000, Clusters found: 4, Noise points: 1710\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1890\n",
      "[DBSCAN] Total points: 2000, Clusters found: 3, Noise points: 1771\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1903\n",
      "\n",
      "Starting reconstruction for molecule 71\n",
      "Ground truth atoms: 13\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 149\n",
      "[DBSCAN] Total points: 1000, Clusters found: 5, Noise points: 41\n",
      "[DBSCAN] Total points: 1000, Clusters found: 4, Noise points: 410\n",
      "[DBSCAN] Total points: 1000, Clusters found: 8, Noise points: 161\n",
      "[DBSCAN] Total points: 1000, Clusters found: 1, Noise points: 457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  40%|████      | 8/20 [23:52<37:44, 188.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 80\n",
      "Ground truth atoms: 14\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1899\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1888\n",
      "[DBSCAN] Total points: 2000, Clusters found: 3, Noise points: 1929\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1975\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 80\n",
      "Ground truth atoms: 14\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 593\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 489\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 585\n",
      "[DBSCAN] Total points: 1000, Clusters found: 3, Noise points: 764\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  45%|████▌     | 9/20 [25:35<29:39, 161.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 81\n",
      "Ground truth atoms: 14\n",
      "[DBSCAN] Total points: 2000, Clusters found: 4, Noise points: 1935\n",
      "[DBSCAN] Total points: 2000, Clusters found: 5, Noise points: 1914\n",
      "[DBSCAN] Total points: 2000, Clusters found: 2, Noise points: 1971\n",
      "[DBSCAN] Total points: 2000, Clusters found: 1, Noise points: 1978\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 81\n",
      "Ground truth atoms: 14\n",
      "[DBSCAN] Total points: 1000, Clusters found: 6, Noise points: 781\n",
      "[DBSCAN] Total points: 1000, Clusters found: 5, Noise points: 745\n",
      "[DBSCAN] Total points: 1000, Clusters found: 2, Noise points: 858\n",
      "[DBSCAN] Total points: 1000, Clusters found: 2, Noise points: 877\n",
      "[DBSCAN] Total points: 1000, Clusters found: 0, Noise points: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理批次:  50%|█████     | 10/20 [31:47<37:48, 226.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting reconstruction for molecule 89\n",
      "Ground truth atoms: 17\n",
      "[DBSCAN] Total points: 2000, Clusters found: 4, Noise points: 1928\n",
      "[DBSCAN] Total points: 2000, Clusters found: 8, Noise points: 1865\n",
      "[DBSCAN] Total points: 2000, Clusters found: 2, Noise points: 1961\n",
      "[DBSCAN] Total points: 2000, Clusters found: 2, Noise points: 1958\n",
      "[DBSCAN] Total points: 2000, Clusters found: 0, Noise points: 2000\n",
      "\n",
      "Starting reconstruction for molecule 89\n",
      "Ground truth atoms: 17\n"
     ]
    }
   ],
   "source": [
    "# GT Field比较主逻辑\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义分批处理函数\n",
    "def process_batch(sample_indices_batch, field_methods, converters, gt_coords, gt_types, visualizer, output_dir):\n",
    "    \"\"\"处理一批样本\"\"\"\n",
    "    batch_results = {\n",
    "        \"field_methods\": field_methods,\n",
    "        \"sample_indices\": sample_indices_batch,\n",
    "        \"comparison_results\": {},\n",
    "        \"summary_statistics\": {}\n",
    "    }\n",
    "    \n",
    "    # 初始化结果结构\n",
    "    for method in field_methods:\n",
    "        batch_results[\"comparison_results\"][method] = {\n",
    "            \"rmsd_values\": [],\n",
    "            \"reconstruction_losses\": [],\n",
    "            \"rmsd_hungarian_values\": [],\n",
    "            \"reconstruction_times\": [],\n",
    "            \"sample_details\": {}\n",
    "        }\n",
    "    \n",
    "    # 对每个样本进行测试\n",
    "    for sample_idx in tqdm(sample_indices_batch, desc=f\"处理批次\", leave=False):\n",
    "        # 获取真实分子信息\n",
    "        gt_mask = (gt_types[sample_idx] != PADDING_INDEX)\n",
    "        gt_valid_coords = gt_coords[sample_idx][gt_mask]\n",
    "        gt_valid_types = gt_types[sample_idx][gt_mask]\n",
    "        \n",
    "        # 对每种field方法进行重建\n",
    "        for method in field_methods:\n",
    "            converter = converters[method]\n",
    "            \n",
    "            # 使用gt field函数\n",
    "            field_func = create_gt_field_func(converter, gt_coords, gt_types, sample_idx)\n",
    "            \n",
    "            # 执行重建\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 使用visualizer的create_reconstruction_animation方法\n",
    "            reconstruction_results = visualizer.create_reconstruction_animation(\n",
    "                gt_coords=gt_coords,\n",
    "                gt_types=gt_types,\n",
    "                converter=converter,\n",
    "                field_func=field_func,\n",
    "                save_interval=100,\n",
    "                animation_name=f\"gt_field_sample_{sample_idx}_{method}\",\n",
    "                sample_idx=sample_idx\n",
    "            )\n",
    "            \n",
    "            reconstruction_time = time.time() - start_time\n",
    "            \n",
    "            # 获取重建结果\n",
    "            recon_coords = reconstruction_results['final_points']\n",
    "            recon_types = reconstruction_results['final_types']\n",
    "            \n",
    "            # 计算RMSD（使用Hungarian算法）\n",
    "            rmsd_hungarian = compute_rmsd_hungarian(gt_valid_coords, recon_coords)\n",
    "            \n",
    "            # 计算重建损失\n",
    "            recon_loss = reconstruction_loss(gt_valid_coords, recon_coords)\n",
    "            \n",
    "            # 保存结果\n",
    "            batch_results[\"comparison_results\"][method][\"rmsd_values\"].append(reconstruction_results['final_rmsd'])\n",
    "            batch_results[\"comparison_results\"][method][\"reconstruction_losses\"].append(recon_loss.item())\n",
    "            batch_results[\"comparison_results\"][method][\"rmsd_hungarian_values\"].append(rmsd_hungarian)\n",
    "            batch_results[\"comparison_results\"][method][\"reconstruction_times\"].append(reconstruction_time)\n",
    "            \n",
    "            # 保存样本详细信息\n",
    "            batch_results[\"comparison_results\"][method][\"sample_details\"][sample_idx] = {\n",
    "                \"rmsd\": reconstruction_results['final_rmsd'],\n",
    "                \"rmsd_hungarian\": rmsd_hungarian,\n",
    "                \"reconstruction_loss\": recon_loss.item(),\n",
    "                \"reconstruction_time\": reconstruction_time,\n",
    "                \"gt_atoms\": len(gt_valid_coords),\n",
    "                \"recon_atoms\": len(recon_coords),\n",
    "                \"gt_types\": gt_valid_types.cpu().numpy().tolist(),\n",
    "                \"recon_types\": recon_types.cpu().numpy().tolist() if len(recon_types) > 0 else [],\n",
    "                \"gif_path\": reconstruction_results['gif_path'],\n",
    "                \"comparison_path\": reconstruction_results['comparison_path']\n",
    "            }\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"recon\"), exist_ok=True)\n",
    "\n",
    "# 创建可视化器\n",
    "visualizer = GNFVisualizer(output_dir)\n",
    "print(f\"输出目录: {output_dir}\")\n",
    "\n",
    "# 存储结果\n",
    "gt_results = {\n",
    "    \"field_methods\": field_methods,\n",
    "    \"sample_indices\": sample_indices,\n",
    "    \"comparison_results\": {},\n",
    "    \"summary_statistics\": {}\n",
    "}\n",
    "\n",
    "# 为每种field方法创建转换器\n",
    "converters = {}\n",
    "for method in field_methods:\n",
    "    print(f\"\\n创建 {method} gt field转换器...\")\n",
    "    # 创建新的配置，修改gradient_field_method\n",
    "    method_config = load_config_from_exp_dir(model_dir)\n",
    "    method_config[\"converter\"][\"gradient_field_method\"] = method\n",
    "    \n",
    "    # 创建转换器\n",
    "    method_converter = create_converter(method_config, device)\n",
    "    converters[method] = method_converter\n",
    "    print(f\"{method} gt field转换器创建完成\")\n",
    "\n",
    "print(\"\\n开始gt field比较...\")\n",
    "\n",
    "# 选择处理方式：分批处理或一次性处理\n",
    "USE_BATCH_PROCESSING = True  # 设置为True启用分批处理，False则一次性处理所有样本\n",
    "\n",
    "if USE_BATCH_PROCESSING and len(sample_indices) > BATCH_SIZE:\n",
    "    print(f\"启用分批处理：将分 {(len(sample_indices) + BATCH_SIZE - 1) // BATCH_SIZE} 批处理\")\n",
    "    \n",
    "    # 分批处理\n",
    "    for i in range(0, len(sample_indices), BATCH_SIZE):\n",
    "        batch_indices = sample_indices[i:i+BATCH_SIZE]\n",
    "        print(f\"\\n处理第 {i//BATCH_SIZE + 1} 批，样本 {i} 到 {i+len(batch_indices)-1}\")\n",
    "        \n",
    "        batch_results = process_batch(batch_indices, field_methods, converters, gt_coords, gt_types, visualizer, output_dir)\n",
    "        \n",
    "        # 合并结果到主结果中\n",
    "        for method in field_methods:\n",
    "            if method not in gt_results[\"comparison_results\"]:\n",
    "                gt_results[\"comparison_results\"][method] = {\n",
    "                    \"rmsd_values\": [],\n",
    "                    \"reconstruction_losses\": [],\n",
    "                    \"rmsd_hungarian_values\": [],\n",
    "                    \"reconstruction_times\": [],\n",
    "                    \"sample_details\": {}\n",
    "                }\n",
    "            \n",
    "            gt_results[\"comparison_results\"][method][\"rmsd_values\"].extend(batch_results[\"comparison_results\"][method][\"rmsd_values\"])\n",
    "            gt_results[\"comparison_results\"][method][\"reconstruction_losses\"].extend(batch_results[\"comparison_results\"][method][\"reconstruction_losses\"])\n",
    "            gt_results[\"comparison_results\"][method][\"rmsd_hungarian_values\"].extend(batch_results[\"comparison_results\"][method][\"rmsd_hungarian_values\"])\n",
    "            gt_results[\"comparison_results\"][method][\"reconstruction_times\"].extend(batch_results[\"comparison_results\"][method][\"reconstruction_times\"])\n",
    "            gt_results[\"comparison_results\"][method][\"sample_details\"].update(batch_results[\"comparison_results\"][method][\"sample_details\"])\n",
    "        \n",
    "        # 每批保存一次结果（可选）\n",
    "        batch_file = os.path.join(output_dir, f\"batch_{i//BATCH_SIZE + 1}_results.json\")\n",
    "        with open(batch_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(batch_results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"第 {i//BATCH_SIZE + 1} 批结果已保存到: {batch_file}\")\n",
    "\n",
    "else:\n",
    "    # 一次性处理所有样本\n",
    "    print(f\"一次性处理所有 {len(sample_indices)} 个样本...\")\n",
    "    \n",
    "    # 对每个样本进行测试 - 使用进度条\n",
    "    for sample_idx in tqdm(sample_indices, desc=\"处理样本\"):\n",
    "        print(f\"\\n处理样本 {sample_idx}...\")\n",
    "        \n",
    "        # 获取真实分子信息\n",
    "        gt_mask = (gt_types[sample_idx] != PADDING_INDEX)\n",
    "        gt_valid_coords = gt_coords[sample_idx][gt_mask]\n",
    "        gt_valid_types = gt_types[sample_idx][gt_mask]\n",
    "        \n",
    "        print(f\"真实原子数: {len(gt_valid_coords)}\")\n",
    "        print(f\"真实原子类型: {gt_valid_types.cpu().numpy()}\")\n",
    "        \n",
    "        # 对每种field方法进行重建\n",
    "        for method in field_methods:\n",
    "            print(f\"\\n--- 测试 {method} gt field方法 ---\")\n",
    "            \n",
    "            converter = converters[method]\n",
    "            \n",
    "            # 使用gt field函数\n",
    "            field_func = create_gt_field_func(converter, gt_coords, gt_types, sample_idx)\n",
    "            \n",
    "            # 执行重建\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 使用visualizer的create_reconstruction_animation方法\n",
    "            reconstruction_results = visualizer.create_reconstruction_animation(\n",
    "                gt_coords=gt_coords,\n",
    "                gt_types=gt_types,\n",
    "                converter=converter,\n",
    "                field_func=field_func,\n",
    "                save_interval=100,\n",
    "                animation_name=f\"gt_field_sample_{sample_idx}_{method}\",\n",
    "                sample_idx=sample_idx\n",
    "            )\n",
    "            \n",
    "            reconstruction_time = time.time() - start_time\n",
    "            \n",
    "            # 获取重建结果\n",
    "            recon_coords = reconstruction_results['final_points']\n",
    "            recon_types = reconstruction_results['final_types']\n",
    "            \n",
    "            # 计算RMSD（使用Hungarian算法）\n",
    "            rmsd_hungarian = compute_rmsd_hungarian(gt_valid_coords, recon_coords)\n",
    "            \n",
    "            # 计算重建损失\n",
    "            recon_loss = reconstruction_loss(gt_valid_coords, recon_coords)\n",
    "            \n",
    "            # 保存结果\n",
    "            if method not in gt_results[\"comparison_results\"]:\n",
    "                gt_results[\"comparison_results\"][method] = {\n",
    "                    \"rmsd_values\": [],\n",
    "                    \"reconstruction_losses\": [],\n",
    "                    \"rmsd_hungarian_values\": [],\n",
    "                    \"reconstruction_times\": [],\n",
    "                    \"sample_details\": {}\n",
    "                }\n",
    "            \n",
    "            gt_results[\"comparison_results\"][method][\"rmsd_values\"].append(reconstruction_results['final_rmsd'])\n",
    "            gt_results[\"comparison_results\"][method][\"reconstruction_losses\"].append(recon_loss.item())\n",
    "            gt_results[\"comparison_results\"][method][\"rmsd_hungarian_values\"].append(rmsd_hungarian)\n",
    "            gt_results[\"comparison_results\"][method][\"reconstruction_times\"].append(reconstruction_time)\n",
    "            \n",
    "            # 保存样本详细信息\n",
    "            gt_results[\"comparison_results\"][method][\"sample_details\"][sample_idx] = {\n",
    "                \"rmsd\": reconstruction_results['final_rmsd'],\n",
    "                \"rmsd_hungarian\": rmsd_hungarian,\n",
    "                \"reconstruction_loss\": recon_loss.item(),\n",
    "                \"reconstruction_time\": reconstruction_time,\n",
    "                \"gt_atoms\": len(gt_valid_coords),\n",
    "                \"recon_atoms\": len(recon_coords),\n",
    "                \"gt_types\": gt_valid_types.cpu().numpy().tolist(),\n",
    "                \"recon_types\": recon_types.cpu().numpy().tolist() if len(recon_types) > 0 else [],\n",
    "                \"gif_path\": reconstruction_results['gif_path'],\n",
    "                \"comparison_path\": reconstruction_results['comparison_path']\n",
    "            }\n",
    "            \n",
    "            print(f\"样本 {sample_idx} - {method} gt field:\")\n",
    "            print(f\"  RMSD: {reconstruction_results['final_rmsd']:.4f}\")\n",
    "            print(f\"  RMSD_Hungarian: {rmsd_hungarian:.4f}\")\n",
    "            print(f\"  Reconstruction Loss: {recon_loss.item():.4f}\")\n",
    "            print(f\"  重建时间: {reconstruction_time:.2f}s\")\n",
    "            print(f\"  真实原子数: {len(gt_valid_coords)}\")\n",
    "            print(f\"  重建原子数: {len(recon_coords)}\")\n",
    "            print(f\"  重建原子类型: {recon_types.cpu().numpy() if len(recon_types) > 0 else 'None'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"gt field比较完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fee572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理完成后的总结\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"批量处理完成！\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 显示处理统计\n",
    "total_samples = len(sample_indices)\n",
    "total_batches = (total_samples + BATCH_SIZE - 1) // BATCH_SIZE if USE_BATCH_PROCESSING else 1\n",
    "\n",
    "print(f\"处理统计:\")\n",
    "print(f\"  总样本数: {total_samples}\")\n",
    "print(f\"  总批次数: {total_batches}\")\n",
    "print(f\"  每批样本数: {BATCH_SIZE}\")\n",
    "print(f\"  处理方式: {'分批处理' if USE_BATCH_PROCESSING and total_samples > BATCH_SIZE else '一次性处理'}\")\n",
    "\n",
    "# 显示结果文件\n",
    "print(f\"\\n生成的文件:\")\n",
    "print(f\"  输出目录: {output_dir}\")\n",
    "print(f\"  重建图片: {output_dir}/recon/\")\n",
    "if USE_BATCH_PROCESSING and total_samples > BATCH_SIZE:\n",
    "    print(f\"  分批结果: {output_dir}/batch_*_results.json\")\n",
    "print(f\"  最终结果: {output_dir}/gt_field_comparison_results.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab82374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在生成gt field比较统计报告...\n",
      "\n",
      "================================================================================\n",
      "GT FIELD方法比较结果汇总\n",
      "================================================================================\n",
      "\n",
      "GAUSSIAN_MAG GT FIELD方法:\n",
      "  RMSD (原始): 0.7208 ± 0.0000\n",
      "  RMSD (Hungarian): 0.0002 ± 0.0000\n",
      "  Reconstruction Loss: 0.0005 ± 0.0000\n",
      "  重建时间: 57.89 ± 0.00s\n",
      "\n",
      "TANH GT FIELD方法:\n",
      "  RMSD (原始): 0.1667 ± 0.0000\n",
      "  RMSD (Hungarian): 0.0046 ± 0.0000\n",
      "  Reconstruction Loss: 0.0096 ± 0.0000\n",
      "  重建时间: 41.35 ± 0.00s\n",
      "\n",
      "================================================================================\n",
      "GT FIELD方法比较:\n",
      "================================================================================\n",
      "RMSD (Hungarian) 比较:\n",
      "  gaussian_mag: 0.0002\n",
      "  tanh: 0.0046\n",
      "  → gaussian_mag gt field 更好 (低 0.0044)\n",
      "\n",
      "Reconstruction Loss 比较:\n",
      "  gaussian_mag: 0.0005\n",
      "  tanh: 0.0096\n",
      "  → gaussian_mag gt field 更好 (低 0.0091)\n",
      "\n",
      "gt field比较结果已保存到: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/funcmol/field_set/gt_field_comparison_results.json\n",
      "\n",
      "================================================================================\n",
      "GT FIELD比较完成！\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 保存gt field比较结果\n",
    "import numpy as np\n",
    "\n",
    "print(\"正在生成gt field比较统计报告...\")\n",
    "\n",
    "# 计算汇总统计\n",
    "for method in field_methods:\n",
    "    method_results = gt_results[\"comparison_results\"][method]\n",
    "    \n",
    "    # 计算统计指标\n",
    "    rmsd_mean = np.mean(method_results[\"rmsd_values\"])\n",
    "    rmsd_std = np.std(method_results[\"rmsd_values\"])\n",
    "    rmsd_hungarian_mean = np.mean(method_results[\"rmsd_hungarian_values\"])\n",
    "    rmsd_hungarian_std = np.std(method_results[\"rmsd_hungarian_values\"])\n",
    "    loss_mean = np.mean(method_results[\"reconstruction_losses\"])\n",
    "    loss_std = np.std(method_results[\"reconstruction_losses\"])\n",
    "    time_mean = np.mean(method_results[\"reconstruction_times\"])\n",
    "    time_std = np.std(method_results[\"reconstruction_times\"])\n",
    "    \n",
    "    gt_results[\"summary_statistics\"][method] = {\n",
    "        \"rmsd_mean\": float(rmsd_mean),\n",
    "        \"rmsd_std\": float(rmsd_std),\n",
    "        \"rmsd_hungarian_mean\": float(rmsd_hungarian_mean),\n",
    "        \"rmsd_hungarian_std\": float(rmsd_hungarian_std),\n",
    "        \"loss_mean\": float(loss_mean),\n",
    "        \"loss_std\": float(loss_std),\n",
    "        \"time_mean\": float(time_mean),\n",
    "        \"time_std\": float(time_std)\n",
    "    }\n",
    "\n",
    "# 打印汇总结果\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GT FIELD方法比较结果汇总\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method in field_methods:\n",
    "    stats = gt_results[\"summary_statistics\"][method]\n",
    "    print(f\"\\n{method.upper()} GT FIELD方法:\")\n",
    "    print(f\"  RMSD (原始): {stats['rmsd_mean']:.4f} ± {stats['rmsd_std']:.4f}\")\n",
    "    print(f\"  RMSD (Hungarian): {stats['rmsd_hungarian_mean']:.4f} ± {stats['rmsd_hungarian_std']:.4f}\")\n",
    "    print(f\"  Reconstruction Loss: {stats['loss_mean']:.4f} ± {stats['loss_std']:.4f}\")\n",
    "    print(f\"  重建时间: {stats['time_mean']:.2f} ± {stats['time_std']:.2f}s\")\n",
    "\n",
    "# 比较两种方法\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"GT FIELD方法比较:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gaussian_stats = gt_results[\"summary_statistics\"][\"gaussian_mag\"]\n",
    "tanh_stats = gt_results[\"summary_statistics\"][\"tanh\"]\n",
    "\n",
    "print(f\"RMSD (Hungarian) 比较:\")\n",
    "print(f\"  gaussian_mag: {gaussian_stats['rmsd_hungarian_mean']:.4f}\")\n",
    "print(f\"  tanh: {tanh_stats['rmsd_hungarian_mean']:.4f}\")\n",
    "if gaussian_stats['rmsd_hungarian_mean'] < tanh_stats['rmsd_hungarian_mean']:\n",
    "    print(f\"  → gaussian_mag gt field 更好 (低 {tanh_stats['rmsd_hungarian_mean'] - gaussian_stats['rmsd_hungarian_mean']:.4f})\")\n",
    "else:\n",
    "    print(f\"  → tanh gt field 更好 (低 {gaussian_stats['rmsd_hungarian_mean'] - tanh_stats['rmsd_hungarian_mean']:.4f})\")\n",
    "\n",
    "print(f\"\\nReconstruction Loss 比较:\")\n",
    "print(f\"  gaussian_mag: {gaussian_stats['loss_mean']:.4f}\")\n",
    "print(f\"  tanh: {tanh_stats['loss_mean']:.4f}\")\n",
    "if gaussian_stats['loss_mean'] < tanh_stats['loss_mean']:\n",
    "    print(f\"  → gaussian_mag gt field 更好 (低 {tanh_stats['loss_mean'] - gaussian_stats['loss_mean']:.4f})\")\n",
    "else:\n",
    "    print(f\"  → tanh gt field 更好 (低 {gaussian_stats['loss_mean'] - tanh_stats['loss_mean']:.4f})\")\n",
    "\n",
    "# 保存结果到JSON\n",
    "gt_results_file = os.path.join(output_dir, \"gt_field_comparison_results.json\")\n",
    "with open(gt_results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(gt_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\ngt field比较结果已保存到: {gt_results_file}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"GT FIELD比较完成！\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funcmol_oss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
