use_cnn: True

# CNN 模型架构参数
# hidden_channels: 各层的通道数列表
# 注意：对于 grid_size=5，最多只能有 2 层 (5->2->1)
# 配置 A：平衡配置（推荐）- 更宽但不太深，梯度流动更好
hidden_channels: [128, 256]  # 各层通道数（配置为~22M参数）

# num_layers: 编码器/解码器空间层数（决定下采样次数）
# - 必须 <= floor(log2(grid_size))
# - grid_size=5: max_num_layers=2 (5->2->1)
# - grid_size=8: max_num_layers=3 (8->4->2->1)
num_layers: 2  # 空间层数（下采样次数）

# num_conv_blocks_per_layer: 每个空间层级堆叠的卷积块数量
# - 增加此参数可以在不改变空间尺寸的情况下增加网络深度
# - 总卷积层数 ≈ num_layers * num_conv_blocks_per_layer * 2（编码+解码）+ bottleneck
# - 例如：num_layers=2, num_conv_blocks_per_layer=2 -> 约13层
num_conv_blocks_per_layer: 2  # 每层堆叠的卷积块数（减少深度，改善梯度流动）

# kernel_size: 3D卷积核大小
kernel_size: 3  # 卷积核大小

# time_emb_dim: 时间嵌入维度（仅用于DDPM模式，即diffusion_method为"new"或"new_x0"时）
time_emb_dim: 256  # 时间嵌入维度（DDPM 模式）

# dropout: Dropout率
dropout: 0.1  # Dropout 率

# 可选配置（不同规模）
# 小模型配置（适合显存较小的GPU）：
# hidden_channels: [32, 64, 128]
# num_layers: 2
# time_emb_dim: 128
# dropout: 0.1

# 中等模型配置（推荐）：
# hidden_channels: [64, 128, 256]
# num_layers: 3
# time_emb_dim: 256
# dropout: 0.1

# 大模型配置（需要大显存）：
# hidden_channels: [64, 128, 256, 512]
# num_layers: 4
# time_emb_dim: 256
# dropout: 0.15

# 网络深度计算（当前配置 A）：
# - 输入投影: 1 层
# - 编码器: num_layers * num_conv_blocks_per_layer = 2 * 2 = 4 层
# - 瓶颈: num_conv_blocks_per_layer + 1 = 3 层
# - 解码器: num_layers * num_conv_blocks_per_layer = 2 * 2 = 4 层
# - 输出投影: 1 层
# - 总计: 约 13 层卷积（有残差连接，梯度流动好）
# - 参数量: 约 22M（code_dim=256）

# 注意事项：
# 1. 3D CNN比GNN消耗更多显存，可能需要减小batch_size
# 2. hidden_channels的长度应该与num_layers匹配
# 3. 时间嵌入维度(time_emb_dim)仅在DDPM模式下使用
# 4. 对于grid_size=8的情况，num_layers不宜超过3（因为下采样会导致特征图太小）
# 5. 增加num_conv_blocks_per_layer可以增加深度而不改变空间尺寸（推荐用于增加模型容量）
