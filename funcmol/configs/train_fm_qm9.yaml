defaults:
  - dset: qm9
  - encoder: gnn
  - decoder: decoder_global
  - denoiser: gnn
  - wjs: wjs_training_small
  - converter: gnf_converter_qm9
  - override hydra/job_logging: custom
  - _self_

debug: False
wandb: False
seed: 1234
# nf_pretrained_path: ../exps/neural_field/nf_qm9
# nf_pretrained_path: ../exps/neural_field/nf_qm9_20250812_181425_750832
nf_pretrained_path: /datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20250911/lightning_logs/version_1/checkpoints/model-epoch=39.ckpt
exp_dir: ../exps/funcmol
exp_name: "fm_${dset.dset_name}/${now:%Y%m%d}"
dirname: "${exp_dir}/${exp_name}"
reload_model_path: null

# data params
smooth_sigma: 0.5 # 2.0, 1.0, 0.5
normalize_codes: 1
on_the_fly: False  # use True to compute codes on the fly from neural field
codes_dir: "/datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20250911/lightning_logs/version_1/checkpoints/codes"

# training params
num_epochs: 50000
dset:
  batch_size: 32
ckpt_every_n_epochs: 5  # 每隔多少epoch保存一次ckpt
decoder:
  code_dim: 128
num_augmentations: null

# optim params
lr: 1e-4
use_lr_schedule: 0 # 0-NOT use lr schedule, 1-use lr schedule
num_warmup_iter: 4000
wd: 1e-2 # NOTE: smaller
ema_decay: 0.999
max_grad_norm: 1.0

# set the running dir
hydra:
  run:
    dir: ${dirname}
  job:
    chdir: false
  output_subdir: null
