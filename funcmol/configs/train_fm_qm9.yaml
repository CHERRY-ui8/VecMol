defaults:
  - dset: qm9
  - encoder: gnn_large
  - decoder: decoder_global_large
  - denoiser: gnn_large
  - converter: gnf_converter_qm9
  - override hydra/job_logging: custom
  - _self_

debug: False
wandb: False
seed: 1234

diffusion_method: "new_x0"  # "old" (WJS) 或 "new" (DDPM) 或 "new_x0" (DDPM with x0)

# DDPM配置（仅当diffusion_method为"new"或"new_x0"时使用）
# 注意：模型参数（hidden_dim, num_layers, time_emb_dim, dropout）已移至 denoiser/gnn.yaml
# 这里只保留扩散过程相关的参数
ddpm:
  # 扩散过程配置
  # 根据 num_timesteps 选择对应的 beta 配置：
  # - num_timesteps: 100   -> beta_start: 0.0005, beta_end: 0.05
  # - num_timesteps: 1000 -> beta_start: 0.0001, beta_end: 0.02
  num_timesteps: 1000 # 1000, 100
  # beta_start: 0.0001  # 当 num_timesteps: 100 时使用0.0005；当 num_timesteps: 1000 时改为 0.0001
  # beta_end: 0.02      # 当 num_timesteps: 100 时使用0.05；当 num_timesteps: 1000 时改为 0.02
  beta_start: 0.0001
  beta_end: 0.02
  schedule: "cosine"  # 可选: "linear", "cosine", "sigmoid"
  
  # Cosine调度参数（仅当schedule为"cosine"时使用）
  # s: 0.008  # cosine调度的偏移参数，控制余弦曲线的偏移，影响噪声添加的曲线形状（s越小曲线越陡峭，s越大曲线越平缓）
  s: 0.008

  # Sigmoid调度参数（仅当schedule为"sigmoid"时使用）
  s1: 0.008
  sT: 0.008
  w: 1.0
  
  # 损失权重配置（仅当diffusion_method为"new_x0"时使用）
  use_time_weight: False  # 是否使用时间步权重 1 + num_timesteps/(t+1)

# NOTE: 位置权重配置
# 根据每个grid坐标附近原子的数量计算权重，对重要位置的codes在loss计算时加大权重
position_weight:
  enabled: False  # 是否启用位置权重
  radius: 3.0     # 计算附近原子的半径阈值（单位：埃） 2, 3
  alpha: 0.5      # 权重系数：weight = 1 + alpha * num_atoms
 # 也用过权重系数：weight = 0.1 + alpha * num_atoms

# NOTE: 联合微调配置（decoder + denoiser）
# 启用后，decoder 和 denoiser 将同时优化，encoder 始终冻结
# decoder 默认使用 neural field checkpoint 中的 decoder，如果从 reload_model_path 加载的 checkpoint 中包含 decoder_state_dict，则优先使用
joint_finetune:
  enabled: False  # 是否启用联合微调（同时优化 decoder 和 denoiser）
  decoder_loss_weight: 10.0  # decoder 损失权重（λ），用于组合损失：total_loss = denoiser_loss + λ * decoder_loss
  decoder_lr: 1e-4  # decoder 的学习率，如果为 null 则使用 denoiser 的学习率（lr）

  num_timesteps_for_decoder: 10  # 从[0, num_timesteps_for_decoder)中随机均匀采样一个timestep进行decoder loss计算（避免OOM）

  n_points: null  # joint fine-tuning时使用的query points数量（如果未设置，则使用dset.n_points，默认500）
  targeted_sampling_ratio: null  # grid点和邻近点的比例 (n_grid / n_neighbor)，null表示使用dset.targeted_sampling_ratio
  atom_distance_threshold: null  # 只采样距离原子多少Å内的query points（单位：Å），null表示使用dset.atom_distance_threshold
  atom_distance_scale: null  # 原子距离衰减尺度（用于loss weighting的指数衰减），null表示使用loss_weighting.atom_distance_scale或默认值0.5
  
  use_cosine_loss: False  # 是否使用余弦距离loss（替代MSE loss）
  magnitude_loss_weight: 0.1  # magnitude loss的权重（用于余弦距离+magnitude loss）


# NOTE: Field Loss Finetune配置（decoder冻结）
# 启用后，decoder参数保持冻结，只用于提供field loss监督信号给denoiser
# query points采样方法：使用dataset的_get_xs方法，一部分采样neighbor，一部分采样grid点
field_loss_finetune:
  enabled: False  # 是否启用field loss finetune
  field_loss_weight: 100.0  # field loss的权重（λ），用于组合损失：total_loss = denoiser_loss + λ * field_loss
  num_timesteps_for_field_loss: 500  # 从[0, num_timesteps_for_field_loss)中随机均匀采样一个timestep进行field loss计算（避免OOM）

# nf_pretrained_path: ../exps/neural_field/nf_qm9
# nf_pretrained_path: ../exps/neural_field/nf_qm9_20250812_181425_750832
nf_pretrained_path: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20260124/lightning_logs/version_0/checkpoints/last.ckpt
exp_dir: ../exps/funcmol
exp_name: "fm_${dset.dset_name}/${now:%Y%m%d}"
dirname: "${exp_dir}/${exp_name}"

reload_model_path: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/funcmol/fm_qm9/20260125/lightning_logs/version_0/checkpoints/last.ckpt
# reload_model_path: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/funcmol/fm_qm9/20260124/lightning_logs/version_1/checkpoints/last.ckpt
# reload_model_path: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/funcmol/fm_qm9/20260120/lightning_logs/version_2/checkpoints/last.ckpt
# reload_model_path: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/funcmol/fm_qm9/20260105/lightning_logs/version_1/checkpoints
  # Funcmol (denoiser) checkpoint 路径，支持两种格式：
  # 1. Lightning checkpoint 文件路径（.ckpt 文件），例如：/path/to/model-epoch=100.ckpt
  # 2. 目录路径，会自动查找 last.ckpt 或最新的 .ckpt 文件，例如：/path/to/checkpoint_dir

# data params
smooth_sigma: 0.5 # 2.0, 1.0, 0.5
normalize_codes: 1
on_the_fly: False  # use True to compute codes on the fly from neural field

# Codes directory configuration
# NOTE：选择是否使用数据增强的codes进行训练
use_augmented_codes: False  # True: 使用数据增强的codes, False: 使用原始codes（不使用数据增强）

# 数据增强版本选择
# 指定要使用的数据增强数量（例如：3 表示使用 aug3 版本，5 表示使用 aug5 版本）
# 如果设置为 null，系统会自动从文件名推断（优先从 LMDB 文件名推断）
num_augmentations: 1  # null 或具体数字（如 3, 5, 10 等）

# 不使用数据增强的codes路径（原始codes）
codes_dir_no_aug: "/datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20260124/lightning_logs/version_0/checkpoints/code_no_aug/"
# codes_dir_no_aug: "/datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20251121/lightning_logs/version_1/checkpoints/code_no_aug/"

# 使用数据增强的codes路径（包含多个增强版本的codes）
codes_dir_with_aug: "/datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20260124/lightning_logs/version_0/checkpoints/codes_no_shuffle/" 
# codes_dir_with_aug: "/datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20251121/lightning_logs/version_1/checkpoints/codes_no_shuffle/"

# training params
num_epochs: 50000
num_iterations: 50000  # 总迭代次数，用于学习率调度
dset:
  batch_size: 44
  num_workers: 4

ckpt_every_n_epochs: 1  # 每隔多少epoch保存一次ckpt
decoder:
  code_dim: 384 # 和encoder的code_dim一致

# optim params
lr: 2e-4 # 1e-3 # 5e-4
# use_lr_schedule: 1 # 0-NOT use lr schedule, 1-use lr schedule
use_lr_schedule: 0 # 暂时禁用学习率调度，使用固定学习率
# num_warmup_iter: 4000
num_warmup_iter: 1000
wd: 1e-6
# ema_decay: 0.999
ema_decay: 0.999  # 启用EMA
max_grad_norm: 0.5  # 0.5，1

# set the running dir
hydra:
  run:
    dir: "${dirname}"
  job:
    chdir: false
  output_subdir: null
