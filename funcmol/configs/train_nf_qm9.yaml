defaults:
  - dset: qm9
  - converter: gnf_converter_qm9
  - encoder: gnn_medium # gnn
  - decoder: decoder_global_medium # decoder_global
  - override hydra/job_logging: custom
  - _self_

debug: False
debug_one_mol: False
debug_subset: False
wandb: False
seed: 1234
exp_dir: ../exps/neural_field
# exp_name: "nf_${dset.dset_name}/${now:%Y%m%d}/${now:%H%M%S}_${now:%f}"
exp_name: "nf_${dset.dset_name}/${now:%Y%m%d}"
dirname: "${exp_dir}/${exp_name}"

# For continue training and finetune decoder
# 使用 finetune_decoder 时，需要设置 reload_model_path 指向包含已训练 decoder 权重的 checkpoint
# 该 checkpoint 会同时加载 encoder 和 decoder 的权重，然后冻结 encoder，只微调 decoder
# reload_model_path: "/datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20251121/lightning_logs/version_1/checkpoints/model-epoch=999.ckpt"
# reload_model_path: "/datapool/data3/storage/pengxingang/pxg/hyc/funcmol-main-neuralfield/exps/neural_field/nf_qm9/20260115/lightning_logs/version_0/checkpoints/model-epoch=212.ckpt"
reload_model_path: null

auto_resume: True  # 是否自动加载最新的checkpoint继续训练

# training params
ckpt_every_n_epochs: 1  # 每隔多少epoch保存一次ckpt
n_epochs: 500
eval_every: 1

# Learning rate scheduler configuration
lr_decay: True  # 启用学习率衰减，防止后期loss爆炸
lr_scheduler_type: "plateau"  # "plateau" (ReduceLROnPlateau) 或 "multistep" (MultiStepLR)

# Warmup configuration
warmup:
  enabled: False  # 是否启用学习率 warmup
  warmup_steps: 10000  # warmup 的步数（steps），学习率从 0 线性增长到目标学习率

# ReduceLROnPlateau 参数（当 lr_scheduler_type="plateau" 时使用）
# 智能学习率调整：当验证损失不再下降时自动降低学习率
lr_factor: 0.5  # 学习率衰减因子（每次降低为原来的0.5倍）
lr_patience: 5  # 等待多少个epoch验证损失没有改善才降低学习率
lr_min: 1e-6  # 最小学习率下限
lr_mode: "min"  # "min" 表示监控指标下降（val_loss越小越好）

# MultiStepLR 参数（当 lr_scheduler_type="multistep" 时使用）
lr_milestones: [30, 60, 90]  # 在这些epoch降低学习率
lr_gamma: 0.5  # 每次衰减为原来的0.5倍（比默认0.1更温和）


# Fine-tuning settings: freeze encoder and only train decoder
# finetune_decoder:
#   enabled: False  # 是否启用decoder微调模式（冻结encoder，只训练decoder）
#   freeze_encoder: True  # 是否冻结encoder（在微调模式下自动为True）
#   # 使用denoiser生成codes（替代encoder）
#   use_denoiser_for_codes: False  # 是否使用denoiser生成codes（如果为True，需要指定denoiser_checkpoint_path）
#   denoiser_checkpoint_path: /datapool/data2/home/pxg/data/hyc/funcmol-main-neuralfield/exps/funcmol/fm_qm9/20251225/lightning_logs/version_1/checkpoints
#   # 当 use_denoiser_for_codes=False 时，使用 on-the-fly 模式：
#   # 实时从冻结的 encoder 计算 codes（类似于 diffusion 训练的 on_the_fly=True）
#   # 微调模式下的特殊精细配置
#   sample_near_atoms_only: True  # 是否只在采样时采样原子附近的点（微调模式下推荐True）
#   atom_distance_threshold: 0.5  # 只采样距离原子多少Å内的query points（单位：Å，可以设置较小值如0.3-0.5）
#   n_points: 300  # query points数量（如果未设置，则使用dset.n_points，默认500）
#   use_cosine_loss: False  # 是否使用余弦距离loss（False表示使用MSE loss）
#   magnitude_loss_weight: 0.1  # magnitude loss的权重（用于余弦距离+magnitude loss），只用余弦距离可能数值不稳定
#   max_timestep_for_decoder: 1  # 使用很小的timestep范围（0到max_timestep_for_decoder-1）进行轻微加噪，然后去噪。这样codes只是经过轻微扰动，去噪后应该接近原始codes，decoder应该能够从这些codes得到和原先ground truth近似的field
#   code_augmentation:
#     enabled: False  # 微调模式下自动启用codes扰动
#     noise_std: 0.003  # 高斯噪声标准差

# DEPRECATED: code augmentation for training robustness (仅在非微调模式下、重新训练时使用)
# code_augmentation:
#   enabled: False  # 是否启用codes扰动
#   noise_std: 0.01  # 高斯噪声标准差

# Loss weighting: 对更重要的区域（原子附近）加权
loss_weighting:
  enabled: True  # 是否启用loss加权
  atom_distance_scale: 1  # 原子距离衰减尺度（越小，距离衰减越快）
  # 分别计算grid点和邻近点的loss时的权重
  grid_loss_weight: 1.0  # grid点loss的权重
  neighbor_loss_weight: 1.0  # 邻近点loss的权重

# set the running dir
hydra:
  run:
    dir: ${dirname}

