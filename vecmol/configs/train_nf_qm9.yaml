defaults:
  - dset: qm9
  - converter: gnf_converter_qm9
  - encoder: gnn_large # gnn
  - decoder: decoder_global_large # decoder_global
  - override hydra/job_logging: custom
  - _self_

debug: False
debug_one_mol: False
debug_subset: False
wandb: False
seed: 1234
exp_dir: ../exps/neural_field
# exp_name: "nf_${dset.dset_name}/${now:%Y%m%d}/${now:%H%M%S}_${now:%f}"
exp_name: "nf_${dset.dset_name}/${now:%Y%m%d}"
dirname: "${exp_dir}/${exp_name}"

# For continue training and finetune decoder
# 使用 finetune_decoder 时，需要设置 reload_model_path 指向包含已训练 decoder 权重的 checkpoint
# 该 checkpoint 会同时加载 encoder 和 decoder 的权重，然后冻结 encoder，只微调 decoder
# reload_model_path: "/data/huayuchen/Neurl-voxel/exps/neural_field/nf_qm9/20251121/lightning_logs/version_1/checkpoints/model-epoch=999.ckpt"
# reload_model_path: "/datapool/data3/storage/pengxingang/pxg/hyc/vecmol-main-neuralfield/exps/neural_field/nf_qm9/20260116/lightning_logs/version_0/checkpoints/model-epoch=275.ckpt"
reload_model_path: /data/huayuchen/Neurl-voxel/exps/neural_field/nf_qm9/20260124/lightning_logs/version_1/checkpoints/last.ckpt

auto_resume: True  # 是否自动加载最新的checkpoint继续训练

# training params
ckpt_every_n_epochs: 1  # 每隔多少epoch保存一次ckpt
n_epochs: 1000
eval_every: 1

# Learning rate scheduler configuration
lr_decay: True  # 启用学习率衰减，防止后期loss爆炸
lr_scheduler_type: "plateau"  # "plateau" (ReduceLROnPlateau) 或 "multistep" (MultiStepLR)

# Warmup configuration
warmup:
  enabled: False  # 是否启用学习率 warmup
  warmup_steps: 10000  # warmup 的步数（steps），学习率从 0 线性增长到目标学习率

# ReduceLROnPlateau 参数（当 lr_scheduler_type="plateau" 时使用）
# 智能学习率调整：当验证损失不再下降时自动降低学习率
lr_factor: 0.8  # 学习率衰减因子（每次降低为原来的0.5倍）
lr_patience: 5  # 等待多少个epoch验证损失没有改善才降低学习率
lr_min: 1e-6  # 最小学习率下限
lr_mode: "min"  # "min" 表示监控指标下降（val_loss越小越好）

# MultiStepLR 参数（当 lr_scheduler_type="multistep" 时使用）
lr_milestones: [30, 60, 90]  # 在这些epoch降低学习率
lr_gamma: 0.5  # 每次衰减为原来的0.5倍（比默认0.1更温和）


# Fine-tuning settings: freeze encoder and only train decoder
finetune_decoder:
  enabled: False  # 是否启用decoder微调模式（冻结encoder，只训练decoder）
  freeze_encoder: False  # 是否冻结encoder（True=只训练decoder，False=同时训练encoder和decoder）
  # 使用denoiser生成codes（替代encoder）
  use_denoiser_for_codes: False  # 是否使用denoiser生成codes（如果为True，需要指定denoiser_checkpoint_path）。注意：如果denoiser是用旧的code_dim训练的，会导致维度不匹配
  denoiser_checkpoint_path: null
  # denoiser_checkpoint_path: /data/huayuchen/Neurl-voxel/exps/vecmol/fm_qm9/20260120/lightning_logs/version_2/checkpoints
  # 当 use_denoiser_for_codes=False 时，使用 on-the-fly 模式：
  # 实时从冻结的 encoder 计算 codes（类似于 diffusion 训练的 on_the_fly=True）
  # 微调模式下的特殊精细配置
  sample_near_atoms_only: False  # 是否只在采样时采样原子附近的点（微调模式下推荐True）
  atom_distance_threshold: 1  # 只采样距离原子多少Å内的query points（单位：Å，可以设置较小值如0.3-0.5）
  n_points: 500  # query points数量（如果未设置，则使用dset.n_points，默认500）
  cosine_loss_weight: 1.0  # cosine loss的权重，>0则使用cosine loss，0则不使用。总loss=cosine_loss_weight*cosine_loss+length_loss_weight*length_loss+magnitude_loss
  length_loss_weight: 1.0  # 模长损失（length loss）的权重，计算预测场和真实场的模长之间的MSE损失。可以与cosine_loss和magnitude_loss一起使用。设置为0表示不使用模长损失
  max_timestep_for_decoder: 10  # 每次从[0, max_timestep_for_decoder)中随机均匀采样一个timestep进行轻微加噪，然后去噪。这样codes只是经过轻微扰动，去噪后应该接近原始codes，decoder应该能够从这些codes得到和原先ground truth近似的field
  real_code_ratio: 1  # 真实codes的使用比例（0.0-1.0），剩余比例使用denoised codes。例如：0.5表示50%使用真实codes（直接从encoder编码，不经过diffusion），50%使用denoised codes（经过diffusion去噪）。真实codes和denoised codes会一起随机均匀采样
  code_augmentation:
    enabled: False  # 微调模式下自动启用codes扰动
    noise_std: 0.003  # 高斯噪声标准差

# DEPRECATED: code augmentation for training robustness (仅在非微调模式下、重新训练时使用)
# code_augmentation:
#   enabled: False  # 是否启用codes扰动
#   noise_std: 0.01  # 高斯噪声标准差

# Loss weighting: 对更重要的区域（原子附近）加权
loss_weighting:
  enabled: False  # 是否启用loss加权
  mode: "inverse_square"  # 权重计算模式："exp" (指数衰减 exp(-distance/scale)) 或 "inverse_square" (负平方反比 1/(distance^2+epsilon))
  atom_distance_scale: 0.3  # 原子距离衰减尺度（exp模式：越小，距离衰减越快；inverse_square模式：用于权重归一化）
  inverse_square_epsilon: 1e-3  # inverse_square模式的epsilon值，避免除零（仅在mode="inverse_square"时使用）
  # 分别计算grid点和邻近点的loss时的权重
  grid_loss_weight: 1.0  # grid点loss的权重
  neighbor_loss_weight: 1.0  # 邻近点loss的权重

# set the running dir
hydra:
  run:
    dir: ${dirname}

