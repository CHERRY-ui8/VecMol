\section{Method}
\label{sec:methods}

Figure~\ref{fig:overall_pipeline} provides an overview of the proposed framework. Our approach represents molecules as continuous vector fields defined over three-dimensional space and models their distribution through a two-stage generative process. Given an input molecular structure, we first encode it into a compact latent representation using a neural field autoencoder. The decoder then reconstructs a continuous vector field conditioned on the latent code, from which atomic positions can be recovered at arbitrary spatial resolution. To generate new molecules, we train a denoising diffusion probabilistic model (DDPM) in the latent space and sample novel latent codes, which are subsequently decoded into molecular vector fields and reconstructed into discrete atomic structures.


\begin{figure*}[t]
    \centering
    \vspace{-0.8cm}
    \includegraphics[width=\textwidth]{figures/overall_pipeline.png}
    \vspace{-0.4cm}
    \caption{
    \textbf{Overview of the proposed molecular neural field framework.}
    The figure illustrates two complementary pipelines sharing the same neural field decoder and reconstruction module.
    \textbf{(a) Encoding and reconstruction.}
    An input molecule, represented by atomic coordinates, is encoded by a neural field encoder $E_\phi$
    into a grid-based latent field representation $\mathbf{z} \in \mathbb{R}^{L^3 \times d}$.
    Given a set of spatial query points $Q = \{\mathbf{q}_i\}_{i=1}^m \subset \mathbb{R}^3$,
    the neural field decoder $D_\psi$ predicts a continuous molecular vector field
    $\mathbf{V} = D_\psi(Q, \mathbf{z})$.
    A dedicated reconstruction module then performs gradient-based ascent and merging
    on the predicted vector field to recover discrete atomic coordinates,
    yielding a reconstructed molecule(Section~\ref{sec:reconstruction}).
    \textbf{(b) Generation and reconstruction.}
    A denoising diffusion probabilistic model is trained in the latent field space.
    Starting from Gaussian noise $\mathbf{z}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$,
    the diffusion model iteratively denoises the latent variables to obtain a sampled latent field $\mathbf{z}_0$.
    The sampled latent field is decoded by the same neural field decoder and converted
    into a discrete molecular structure using the same reconstruction module.
    }
    \label{fig:overall_pipeline}
\end{figure*}




\subsection{Neural Field Representation}

We represent a 3D molecule as a continuous vector field that encodes atomic occupancy in space.
Unlike discrete representations such as point clouds or voxel grids, the proposed representation
models molecular geometry as a continuous function, enabling resolution-free reconstruction
and scalable modeling of molecules with varying sizes.

Formally, we define a vector field
$\mathbf{v}: \mathbb{R}^3 \rightarrow \mathbb{R}^{K \times 3}$,
where each spatial query point $\mathbf{q} \in \mathbb{R}^3$
is mapped to $K$ three-dimensional vectors, one for each atom type.
Each vector $\mathbf{v}_k(\mathbf{q})$ points toward the nearest atom of type $k$,
providing a continuous indication of atomic locations.

This representation offers several advantages:
(i) it naturally captures the continuous nature of molecular geometry,
(ii) it allows reconstruction at arbitrary spatial resolution,
(iii) its complexity depends on the field representation rather than the number of atoms,
and (iv) it imposes minimal structural assumptions, making it applicable to diverse molecular systems.

We parameterize the molecular field using a conditional neural field model.
Given a molecule-specific latent code
$\mathbf{z} \in \mathbb{R}^{L^3 \times d}$ defined on a fixed 3D grid of size $L \times L \times L$,
and a set of query points $Q = \{ \mathbf{q}_i \}_{i=1}^m$,
a shared decoder $D_\psi$ predicts the vector field values:
\begin{equation}
\mathbf{V} = D_\psi(Q, \mathbf{z}) \in \mathbb{R}^{m \times K \times 3},
\end{equation}
where $\mathbf{V}[i, k, :] = \mathbf{v}_k(\mathbf{q}_i)$ denotes the predicted vector for atom type $k$ at query point $\mathbf{q}_i$.

Discrete atomic coordinates are recovered by performing gradient ascent
on the predicted vector field, followed by clustering of converged points.
This procedure allows us to reconstruct molecular structures from continuous fields
without relying on predefined grids or atom counts.

\subsection{Field Definition}
\label{sec:field}

To supervise the neural field, we define a ground-truth vector field
derived from discrete molecular structures.
Our field definition explicitly separates direction selection from magnitude control,
which is crucial for stable optimization and accurate reconstruction.

For a query point $\mathbf{q} \in \mathbb{R}^3$ and atom type $k$,
let $\mathcal{A}_k = \{\mathbf{a}_j\}_{j=1}^{n_k}$ denote the set of $n_k$ atoms of type $k$ with coordinates $\mathbf{a}_j \in \mathbb{R}^3$.
We compute distances $d_j = \|\mathbf{q} - \mathbf{a}_j\|$ and direction vectors $\mathbf{d}_j = \mathbf{a}_j - \mathbf{q}$.

Direction selection is performed using softmax weighting over inverse distances:
\begin{equation}
w_j^{\text{softmax}} = \frac{\exp(-d_j / \sigma_{\text{sf}})}{\sum_{j'=1}^{n_k} \exp(-d_{j'} / \sigma_{\text{sf}})},
\end{equation}
where $\sigma_{\text{sf}} > 0$ is a temperature parameter controlling the locality of direction selection.

Magnitude control is achieved using a bounded, differentiable function:
\begin{equation}
w_j^{\text{mag}} = \tanh(d_j / \sigma_{\text{mag}}),
\end{equation}
where $\sigma_{\text{mag}} > 0$ controls the saturation point of the magnitude function.

The normalized direction vector is:
\begin{equation}
\hat{\mathbf{d}}_j = \frac{\mathbf{d}_j}{\|\mathbf{d}_j\| + \epsilon},
\end{equation}
where $\epsilon > 0$ is a small constant for numerical stability.

The ground-truth vector field for atom type $k$ at query point $\mathbf{q}$ is:
\begin{equation}
\mathbf{v}_k^*(\mathbf{q}) = \sum_{j=1}^{n_k} w_j^{\text{softmax}} \cdot w_j^{\text{mag}} \cdot \hat{\mathbf{d}}_j.
\end{equation}

This composite formulation provides several benefits:
(i) the softmax weighting ensures nearby atoms dominate direction selection while maintaining smoothness,
(ii) the tanh magnitude function provides bounded, well-conditioned gradients that prevent numerical instability,
(iii) the separation of direction and magnitude allows independent control of field properties.

We selected $\sigma_{\mathrm{sf}} = 0.1$ and $\sigma_{\mathrm{mag}} = 2.0$ as the optimal parameter configuration through systematic evaluation.
This composite field definition outperforms alternative formulations (e.g., Gaussian or LogSumExp fields),
yielding lower reconstruction error and faster convergence.
Detailed analysis of field variants and parameter selection is provided in Section~\ref{sec:field_analysis}.

\subsection{Neural Field Autoencoder}

The neural field autoencoder consists of an encoder that maps variable-size molecular structures to fixed-size latent codes, and a decoder that reconstructs continuous vector fields from latent codes and spatial queries.

\subsubsection{Encoder: Molecule-to-Field Encoding}

The encoder $E_\phi$ maps a variable-size molecular structure to a fixed-size latent field representation.
Given atomic coordinates $\mathbf{X} \in \mathbb{R}^{n \times 3}$ and atom types
$T \in \{0, \ldots, K-1\}^n$ for a molecule with $n$ atoms,
the encoder produces latent codes $\mathbf{z} \in \mathbb{R}^{L^3 \times d}$ defined on a regular 3D grid of $L^3$ anchor points.

To bridge the variable-size molecular graph and the fixed grid representation,
we adopt a cross-graph encoding architecture with two types of edges:
(i) an intra-atomic graph $\mathcal{G}_a$ connecting each atom to its $k_a$ nearest atomic neighbors via k-NN,
and (ii) a cross graph $\mathcal{G}_{ag}$ connecting each grid anchor point to its $k_g$ nearest atoms.
Additional architectural details are provided in Section~\ref{app:autoencoder}.

Node features are initialized as:
\begin{equation}
\mathbf{h}_i^{(0)} = \begin{cases}
\text{OneHot}(T_i) \in \mathbb{R}^d & \text{if } i \text{ is an atom node} \\
\mathbf{0} \in \mathbb{R}^d & \text{if } i \text{ is a grid node}
\end{cases},
\end{equation}
where atom type encodings are zero-padded to dimension $d$ if $K < d$.

For each message-passing layer $\ell = 1, \ldots, L_e$, we compute messages along edges:
\begin{equation}
\mathbf{m}_{ij}^{(\ell)} = \text{MLP}\left([\mathbf{h}_i^{(\ell-1)}, \mathbf{h}_j^{(\ell-1)}, \phi_d(\|\mathbf{x}_i - \mathbf{x}_j\|)]\right),
\end{equation}
where $\phi_d: \mathbb{R} \rightarrow \mathbb{R}^{d_e}$ is a Gaussian distance expansion function that maps distances to edge features,
and $\mathbf{x}_i, \mathbf{x}_j$ are node coordinates.

Node features are updated via mean aggregation and residual connection:
\begin{equation}
\mathbf{h}_i^{(\ell)} = \text{LayerNorm}\left(\mathbf{h}_i^{(\ell-1)} + \frac{1}{|\mathcal{N}(i)|}\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{(\ell)}\right),
\end{equation}
where $\mathcal{N}(i)$ denotes the neighborhood of node $i$ in the combined graph $\mathcal{G}_a \cup \mathcal{G}_{ag}$.

The final latent code $\mathbf{z} \in \mathbb{R}^{L^3 \times d}$ is obtained by collecting features from all grid anchor nodes:
\begin{equation}
\mathbf{z}[g, :] = \mathbf{h}_{N_a + g}^{(L_e)},
\end{equation}
where $N_a$ is the total number of atom nodes and $g \in \{0, \ldots, L^3-1\}$ indexes grid anchors.
This cross-graph formulation enables scalable encoding while preserving geometric structure.

\subsubsection{Decoder: Field Prediction with Equivariance}

The decoder $D_\psi$ predicts continuous vector fields from latent codes and spatial queries.
Given query points $Q = \{\mathbf{q}_i\}_{i=1}^m$ and latent grid codes $\mathbf{z} \in \mathbb{R}^{L^3 \times d}$,
the decoder outputs $K$ vectors at each query location.

To ensure that the predicted fields respect geometric symmetries,
we implement the decoder using an E(n)-equivariant graph neural network (EGNN).
The decoder constructs a graph where each query point $\mathbf{q}_i$ is connected
to its $k$ nearest grid anchor points via k-NN, enabling local interpolation of the latent field.

Node features are initialized as:
\begin{equation}
\mathbf{h}_i^{(0)} = \begin{cases}
\mathbf{0} \in \mathbb{R}^d & \text{if } i \text{ is a query point} \\
\mathbf{z}[g, :] & \text{if } i \text{ is grid anchor } g
\end{cases}.
\end{equation}

For each EGNN layer $\ell = 1, \ldots, L_d$, we compute edge messages:
\begin{equation}
\mathbf{m}_{ij}^{(\ell)} = \text{EdgeMLP}\left([\mathbf{h}_i^{(\ell-1)}, \mathbf{h}_j^{(\ell-1)}, \|\mathbf{x}_i - \mathbf{x}_j\|]\right),
\end{equation}
where $\mathbf{x}_i, \mathbf{x}_j$ are node coordinates.

Coordinates are updated in an equivariant manner:
\begin{equation}
\mathbf{x}_i^{(\ell)} = \mathbf{x}_i^{(\ell-1)} + \frac{1}{|\mathcal{N}(i)|}\sum_{j \in \mathcal{N}(i)} a_{ij}^{(\ell)} \cdot \frac{\mathbf{x}_j^{(\ell-1)} - \mathbf{x}_i^{(\ell-1)}}{\|\mathbf{x}_j^{(\ell-1)} - \mathbf{x}_i^{(\ell-1)}\| + \epsilon},
\end{equation}
where $a_{ij}^{(\ell)} = \text{CoordMLP}(\mathbf{m}_{ij}^{(\ell)}) \in \mathbb{R}$ is a scalar coefficient,
and a cosine cutoff function is applied to messages beyond a distance threshold.

Node features are updated via:
\begin{equation}
\mathbf{h}_i^{(\ell)} = \mathbf{h}_i^{(\ell-1)} + \text{NodeMLP}\left([\mathbf{h}_i^{(\ell-1)}, \frac{1}{|\mathcal{N}(i)|}\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{(\ell)}]\right).
\end{equation}

After the final layer, the decoder predicts source locations $\mathbf{s}_i^{(k)} \in \mathbb{R}^3$ for each query point $i$ and atom type $k$:
\begin{equation}
\mathbf{s}_i^{(k)} = \mathbf{x}_i^{(L_d)} + \Delta\mathbf{x}_i^{(k)},
\end{equation}
where $\Delta\mathbf{x}_i^{(k)}$ is obtained from the coordinate update of a specialized field prediction layer with output dimension $K$.

The vector field is computed as the displacement from query points to predicted sources:
\begin{equation}
\mathbf{v}_k(\mathbf{q}_i) = \mathbf{s}_i^{(k)} - \mathbf{q}_i.
\end{equation}

This E(n)-equivariant design guarantees that the learned field representation
is invariant to global translations and equivariant to rotations,
ensuring physical consistency and suitability for gradient-based reconstruction.

\subsubsection{Training}

We jointly train the encoder $E_\phi$ and decoder $D_\psi$ to reconstruct
ground-truth vector fields from molecular structures.
For each molecule with atomic coordinates $\mathbf{X}$ and types $T$,
we sample $m$ spatial query points $Q = \{\mathbf{q}_i\}_{i=1}^m$,
encode the molecule to obtain latent codes $\mathbf{z} = E_\phi(\mathbf{X}, T)$,
and predict vector fields $\mathbf{V} = D_\psi(Q, \mathbf{z})$.

The training objective minimizes the mean squared error between predicted and ground-truth fields:
\begin{equation}
\mathcal{L}_{\text{auto}} = \frac{1}{mK}\sum_{i=1}^m \sum_{k=1}^K \|\mathbf{v}_k(\mathbf{q}_i) - \mathbf{v}_k^*(\mathbf{q}_i)\|^2,
\end{equation}
where $\mathbf{v}_k^*(\mathbf{q}_i)$ is the ground-truth field defined in Section~\ref{sec:field}.

We employ adaptive spatial sampling to concentrate query points near atomic centers, where gradients are most informative (see Section~\ref{sec:adaptive_spatial_sampling}).
Optionally, we apply distance-based weighting to emphasize regions near atoms:
\begin{equation}
w_i = \exp(-\min_j \|\mathbf{q}_i - \mathbf{a}_j\| / \sigma_w),
\end{equation}
where $\sigma_w > 0$ controls the weighting scale, leading to a weighted loss:
\begin{equation}
\mathcal{L}_{\text{auto}} = \frac{1}{\sum_i w_i}\sum_{i=1}^m w_i \sum_{k=1}^K \|\mathbf{v}_k(\mathbf{q}_i) - \mathbf{v}_k^*(\mathbf{q}_i)\|^2.
\end{equation}
We address loss imbalance between field supervision and coordinate reconstruction through careful normalization and weighting strategies (see Section~\ref{sec:loss_imbalance}).

\subsection{Latent Diffusion Model}

We model the distribution of molecular structures
by performing generative modeling in the latent field space.
Specifically, we train a denoising diffusion probabilistic model (DDPM)
over latent codes $\mathbf{z}_0 \in \mathbb{R}^{L^3 \times d}$,
whose dimensionality is fixed across molecules.

The forward diffusion process gradually corrupts latent codes with Gaussian noise over $T$ timesteps:
\begin{equation}
q(\mathbf{z}_t | \mathbf{z}_0) = \mathcal{N}(\mathbf{z}_t; \sqrt{\bar{\alpha}_t} \mathbf{z}_0, (1 - \bar{\alpha}_t) \mathbf{I}),
\end{equation}
where $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ and $\alpha_t = 1 - \beta_t$,
with $\beta_t$ following a cosine noise schedule:
\begin{equation}
\bar{\alpha}_t = \frac{\cos((t/T + s) / (1 + s) \cdot \pi/2)^2}{\cos(s / (1 + s) \cdot \pi/2)^2},
\end{equation}
where $s = 0.008$ is a small offset parameter.

The reverse denoising process is learned by a neural network $\epsilon_\theta$:
\begin{equation}
p_\theta(\mathbf{z}_{t-1} | \mathbf{z}_t) = \mathcal{N}(\mathbf{z}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{z}_t, t), \sigma_t^2 \mathbf{I}),
\end{equation}
where the mean is parameterized as:
\begin{equation}
\boldsymbol{\mu}_\theta(\mathbf{z}_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{z}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(\mathbf{z}_t, t)\right),
\end{equation}
and the variance is $\sigma_t^2 = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t$.

The denoiser $\epsilon_\theta$ is parameterized by an EGNN operating on grid anchor points,
with node features initialized as $\mathbf{h}_i^{(0)} = \mathbf{z}_t[i, :]$ and time embeddings $\mathbf{t}_{\text{emb}}$ injected at each layer:
\begin{equation}
\mathbf{m}_{ij}^{(\ell)} = \text{EdgeMLP}\left([\mathbf{h}_i^{(\ell-1)}, \mathbf{h}_j^{(\ell-1)}, \|\mathbf{x}_i - \mathbf{x}_j\|, \mathbf{t}_{\text{emb}}]\right),
\end{equation}
where $\mathbf{t}_{\text{emb}}$ is obtained via sinusoidal position encoding of timestep $t$.
Edges are constructed using radius-based neighborhoods, with detailed ablation studies on the radius parameter provided in Section~\ref{sec:egnn_radius_ablation}.

Operating in latent space significantly reduces computational cost
compared to diffusion directly on atomic coordinates,
while preserving sufficient expressiveness for molecular generation.
Additional details on the diffusion model design are provided in Section~\ref{app:diffusion}.

\subsubsection{Training}

After autoencoder training, the encoder $E_\phi$ is frozen.
Latent codes $\mathbf{z}_0 = E_\phi(\mathbf{X}, T)$ extracted from training molecules are used to train the diffusion model.

The training objective minimizes the prediction error of the noise:
\begin{equation}
\mathcal{L}_{\text{diff}} = \mathbb{E}_{t, \mathbf{z}_0, \boldsymbol{\epsilon}} \left[\|\boldsymbol{\epsilon} - \epsilon_\theta(\sqrt{\bar{\alpha}_t} \mathbf{z}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}, t)\|^2\right],
\end{equation}
where $t \sim \text{Uniform}(1, T)$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$,
and the expectation is taken over the training distribution of molecules.

An exponential moving average of model parameters is maintained to stabilize training:
\begin{equation}
\theta_{\text{EMA}} \leftarrow \lambda \theta_{\text{EMA}} + (1 - \lambda) \theta,
\end{equation}
where $\lambda \in (0, 1)$ is the momentum parameter.

\subsection{Sampling and Molecule Reconstruction}
\label{sec:reconstruction}

Given a predicted vector field $\mathbf{V} = D_\psi(Q, \mathbf{z})$,
we reconstruct discrete molecular structures by iteratively extracting
atomic candidates via gradient ascent and merging.
This procedure corresponds to the reconstruction module illustrated
in Figure~\ref{fig:overall_pipeline}.

To generate molecules, we first sample a latent code
via the reverse diffusion process starting from Gaussian noise $\mathbf{z}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.

The reverse diffusion process iteratively denoises the latent code:
\begin{equation}
\mathbf{z}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{z}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(\mathbf{z}_t, t)\right) + \sigma_t \boldsymbol{\epsilon},
\end{equation}
for $t = T, T-1, \ldots, 1$, where $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ for $t > 1$ and $\boldsymbol{\epsilon} = \mathbf{0}$ for $t = 1$.

The sampled latent code $\mathbf{z}_0$ is then decoded into a continuous vector field $\mathbf{V} = D_\psi(Q, \mathbf{z}_0)$ for query points $Q$.

Discrete atomic positions are recovered by initializing candidate query points
and performing gradient ascent along the predicted vector field for each atom type $k$:
\begin{equation}
\mathbf{q}_i^{(t+1)} = \mathbf{q}_i^{(t)} + \eta \cdot \mathbf{v}_k(\mathbf{q}_i^{(t)}),
\end{equation}
where $\eta > 0$ is the step size, and the process continues until convergence $\|\mathbf{v}_k(\mathbf{q}_i^{(t)})\| < \tau$ for a threshold $\tau > 0$.

Converged points are clustered using DBSCAN with distance threshold $\epsilon$ and minimum samples $m_{\text{min}}$ to obtain atomic coordinates.
Chemical bonds are inferred using standard distance-based heuristics,
and chemically invalid structures are filtered out based on bond length and angle constraints.

This procedure enables end-to-end generation of valid 3D molecular structures
from latent diffusion samples.
The detailed algorithm for field-to-molecule reconstruction is provided in Algorithm~\ref{algo:field_to_molecule} in Section~\ref{sec:field_analysis}.
