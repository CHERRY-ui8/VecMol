\appendix
\section{Additional Details on Field--Molecule Conversion}\label{sec:field_analysis}

\subsection{Recap of the Composite Field Definition}
The gradient neural field (GNF) maps discrete atomic coordinates to a continuous vector field, where each query point $\mathbf{z} \in \mathbb{R}^3$ is assigned a gradient vector that guides optimization toward atomic positions. Formally, given atomic coordinates $\mathbf{X} = \{\mathbf{x}_i\}_{i=1}^{N}$ and query points $\mathbf{Z} = \{\mathbf{z}_j\}_{j=1}^{M}$, the field $\mathbf{g}(\mathbf{z}_j)$ aggregates contributions from all atoms as
\begin{equation}
\mathbf{g}(\mathbf{z}_j) = \sum_{i=1}^{N} w_{ij} \cdot \mathbf{d}_{ij},
\end{equation}
where $\mathbf{d}_{ij} = \mathbf{x}_i - \mathbf{z}_j$ denotes the displacement vector from the query point to atom $i$, and $w_{ij}$ is a distance-dependent weighting function. The specific choice of $w_{ij}$ plays a central role in determining numerical stability, convergence behavior, and robustness in multi-atom configurations.

\subsection{Design Requirements for Field--Molecule Conversion}
Rather than enumerating specific formulations upfront, we first formalize the requirements imposed by the field-to-molecule conversion task. A valid gradient field must satisfy four constraints:

\textbf{R1: Local single-atom dominance.} When a query point is very close to an atom, the gradient field should be dominated by that atom's contribution, with minimal interference from other atoms. This ensures precise convergence to individual atomic positions without ambiguity.

\textbf{R2: Smooth gradient transitions.} The gradient field should vary smoothly across space without abrupt changes or discontinuities. Smooth transitions facilitate accurate learning by neural networks and prevent optimization instabilities.

\textbf{R3: Bounded gradient magnitude.} Gradient magnitudes must remain bounded and well-conditioned, especially near atomic positions. Unbounded or rapidly varying gradients lead to numerical instabilities and make the field difficult for neural networks to learn accurately.

\textbf{R4: Long-range gradient presence.} The gradient field should remain non-vanishing at moderate to large distances from atoms. While not strictly necessary for reconstruction, maintaining informative gradients across the spatial domain improves sampling efficiency by ensuring that query points always receive useful guidance, even when initialized far from atomic centers.

\subsection{Mathematical Formulation of Field Variants}\label{sec:field_variants}
We formalize several representative gradient field constructions evaluated in this work. For a query point $\mathbf{z}$ and atomic coordinates $\{\mathbf{x}_i\}_{i=1}^N$, we define the displacement vector $\mathbf{d}_i = \mathbf{x}_i - \mathbf{z}$ and distance $d_i = \|\mathbf{d}_i\|$. The composite gradient field is obtained by aggregating per-atom contributions,
\begin{equation}
\mathbf{g}(\mathbf{z}) = \sum_{i=1}^{N} \mathbf{g}_i(\mathbf{z}).
\end{equation}
In modeling the gradient field, we explored several formulations that arise naturally from physical intuition. Below we present the mathematical forms of these field variants and analyze their characteristic behaviors with respect to the four design requirements.

\paragraph{Gaussian Field.}
Each atom contributes a distance-weighted gradient,
\begin{equation}
\mathbf{g}_i = \mathbf{d}_i \cdot \frac{\exp(-d_i^2 / (2\sigma^2))}{\sigma^2}.
\end{equation}
While smooth and bounded, this formulation violates R1 (local single-atom dominance): for any finite $\sigma$, $\mathbf{g}_i(\mathbf{z})$ remains non-zero even when $\mathbf{z}$ coincides with another atom $\mathbf{x}_j$. As a result, gradient fields from different atoms overlap and interfere, preventing precise convergence to individual atomic positions. Additionally, the exponential decay causes gradients to vanish rapidly at moderate distances, violating R4.

\paragraph{Softmax Field.}
Distances are normalized across atoms using a softmax,
\begin{equation}
w_i = \frac{\exp(-d_i/T)}{\sum_k \exp(-d_k/T)}, \qquad \mathbf{g}_i = \frac{\mathbf{d}_i}{\|\mathbf{d}_i\|} \cdot w_i.
\end{equation}
Although the softmax encourages nearest-atom dominance, this formulation violates R2 (smooth transitions): symmetric configurations lead to comparable weights for multiple atoms, and when the corresponding displacement vectors point in opposing directions, their contributions cancel, creating discontinuous zero-gradient plateaus that stall optimization. The abrupt transitions between dominance regions further compromise smoothness.

\paragraph{Inverse-Square Field.}
Inspired by Coulomb-like interactions, this formulation assigns
\begin{equation}
w_i = \frac{\text{strength}}{d_i^2 + \epsilon}, \qquad \mathbf{g}_i = \frac{\mathbf{d}_i}{\|\mathbf{d}_i\|} \cdot w_i.
\end{equation}
This approach violates R3 (bounded magnitude): the gradient magnitude diverges as $d_i \to 0$, leading to severe numerical instability near atomic positions. The rapid variation in gradient strength makes the field difficult for neural networks to learn accurately, even with ad-hoc safeguards.

\paragraph{Gaussian Magnitude Field.}
This variant combines softmax-based directional selection with a Gaussian-decayed magnitude term:
\begin{equation}
w_i^{\mathrm{sf}} = \frac{\exp(-d_i/\sigma_{\mathrm{sf}})}{\sum_k \exp(-d_k/\sigma_{\mathrm{sf}})}, \qquad w_i^{\mathrm{mag}} = \exp(-d_i^2 / (2\sigma_{\mathrm{mag}}^2)) \cdot d_i,
\end{equation}
\begin{equation}
\mathbf{g}_i = \frac{\mathbf{d}_i}{\|\mathbf{d}_i\|} \cdot w_i^{\mathrm{sf}} \cdot w_i^{\mathrm{mag}}.
\end{equation}
By incorporating softmax normalization, this formulation addresses R1 (local single-atom dominance), ensuring that nearby atoms dominate the gradient field. However, the Gaussian decay in the magnitude term causes gradients to vanish rapidly at large distances, violating R4. While this approach improves upon the pure Gaussian field, it is not optimal for our application.

\paragraph{Distance Field with Clipping.}
This formulation uses distance directly as the magnitude term, with clipping to bound the gradient:
\begin{equation}
w_i^{\mathrm{sf}} = \frac{\exp(-d_i/\sigma_{\mathrm{sf}})}{\sum_k \exp(-d_k/\sigma_{\mathrm{sf}})}, \qquad w_i^{\mathrm{mag}} = \text{clip}(d_i, 0, d_{\max}),
\end{equation}
\begin{equation}
\mathbf{g}_i = \frac{\mathbf{d}_i}{\|\mathbf{d}_i\|} \cdot w_i^{\mathrm{sf}} \cdot w_i^{\mathrm{mag}}.
\end{equation}
The clipping operation prevents unbounded gradients (satisfying R3) and provides a physically intuitive interpretation where gradient magnitude scales with distance. However, the hard clipping introduces discontinuities in the gradient field, violating R2 (smooth transitions). The abrupt transitions at the clipping boundary make the field difficult for neural networks to learn accurately.

\paragraph{Softmax--Tanh Field (Ours).}
Our proposed formulation decouples directional selection from magnitude control:
\begin{equation}
w_i^{\mathrm{sf}} = \frac{\exp(-d_i/\sigma_{\mathrm{sf}})}{\sum_k \exp(-d_k/\sigma_{\mathrm{sf}})}, \qquad w_i^{\mathrm{mag}} = \tanh(d_i/\sigma_{\mathrm{mag}}),
\end{equation}
\begin{equation}
\mathbf{g}_i = \frac{\mathbf{d}_i}{\|\mathbf{d}_i\|} \cdot w_i^{\mathrm{sf}} \cdot w_i^{\mathrm{mag}}.
\end{equation}
The softmax term ensures smooth single-atom dominance (R1 and R2), while the tanh magnitude term provides bounded, monotonic scaling (R3). The tanh function maintains non-vanishing gradients at moderate distances (R4), though with reduced magnitude. By construction, this field avoids gradient explosion, destructive cancellation, and long-range interference, satisfying all four design requirements simultaneously.

The formulation involves two key hyperparameters: $\sigma_{\mathrm{sf}}$ controls the sharpness of the softmax distribution, determining how quickly the field transitions from multi-atom to single-atom dominance, while $\sigma_{\mathrm{mag}}$ controls the saturation point of the tanh function, affecting the gradient magnitude at different distances. Figure~\ref{fig:tanh_params} illustrates how varying these parameters affects the one-dimensional field behavior, demonstrating the robustness of the formulation across a reasonable parameter range.

We selected $\sigma_{\mathrm{sf}} = 0.1$ and $\sigma_{\mathrm{mag}} = 2.0$ as the optimal parameter configuration. The choice of $\sigma_{\mathrm{sf}} = 0.1$ can be justified through mathematical analysis. Consider a query point at distance $d_A = 0.75$ \AA\ from atom A, with another atom B at distance $d_B = 1.5$ \AA\ (typical C--C bond length). The softmax weights are:
\begin{equation}
w_A = \frac{\exp(-d_A/\sigma_{\mathrm{sf}})}{\exp(-d_A/\sigma_{\mathrm{sf}}) + \exp(-d_B/\sigma_{\mathrm{sf}})}, \quad
w_B = \frac{\exp(-d_B/\sigma_{\mathrm{sf}})}{\exp(-d_A/\sigma_{\mathrm{sf}}) + \exp(-d_B/\sigma_{\mathrm{sf}})}.
\end{equation}
With $\sigma_{\mathrm{sf}} = 0.1$, we have $\exp(-0.75/0.1) \approx 5.53 \times 10^{-4}$ and $\exp(-1.5/0.1) \approx 3.06 \times 10^{-7}$, yielding $w_A \approx 0.9994$ and $w_B \approx 0.0006$. The weight ratio $w_A/w_B \approx 1.8 \times 10^3$ demonstrates strong local single-atom dominance at half the interatomic spacing, satisfying requirement R1 while maintaining smooth transitions (R2). The value $\sigma_{\mathrm{mag}} = 2.0$ positions the tanh saturation point at roughly the typical interatomic distance (around 1.5 \AA), ensuring that gradient magnitudes remain well-conditioned near atoms (R3) while maintaining informative gradients at moderate distances (R4). This parameter combination achieves the best balance between reconstruction accuracy, numerical stability, and convergence efficiency.

\subsection{Visualizing Gradient Fields: A One-Dimensional Perspective}
\label{sec:field_1d_vis}
To develop intuition, we visualize gradient fields in a simplified one-dimensional setting with two atoms placed at $x=-1$ and $x=1$. In one dimension, the field value encodes both direction and magnitude: positive values indicate motion to the right, negative values indicate motion to the left, and zero crossings correspond to equilibrium points. This visualization makes cancellation regions, unbounded gradients, and ineffective long-range behavior immediately apparent, providing a geometric interpretation of the mathematical properties discussed above.
\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{field_methods_1d.png}
\caption{One-dimensional visualization of representative gradient field formulations with two atoms at $x=-1$ and $x=1$. The plots illustrate characteristic behaviors such as gradient cancellation, divergence near atoms, long-range interference, and stable convergence.}
\label{fig:field_1d}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{tanh_params_1d.png}
\caption{One-dimensional visualization of the Softmax--Tanh field under different parameter settings. Up: varying $\sigma_{\mathrm{sf}}$ (softmax sharpness) with fixed $\sigma_{\mathrm{mag}}=2.0$. Down: varying $\sigma_{\mathrm{mag}}$ (magnitude saturation) with fixed $\sigma_{\mathrm{sf}}=0.1$. The field remains well-behaved across a reasonable parameter range, demonstrating robustness of the formulation.}
\label{fig:tanh_params}
\end{figure}
\subsection{Softmax--Tanh Field: Parameterization and Design Rationale}
Our proposed formulation combines softmax-based directional selection with tanh-based magnitude modulation:
\begin{equation}
w_{ij}^{\text{sf}} = \frac{\exp(-d_{ij}/\sigma_{\text{sf}})}{\sum_k \exp(-d_{kj}/\sigma_{\text{sf}})}, \quad w_{ij}^{\text{mag}} = \tanh(d_{ij}/\sigma_{\text{mag}}).
\end{equation}
\begin{equation}
\mathbf{g}(\mathbf{z}_j) = \sum_i \frac{\mathbf{d}_{ij}}{\|\mathbf{d}_{ij}\|} \cdot w_{ij}^{\text{sf}} \cdot w_{ij}^{\text{mag}}.
\end{equation}
The softmax term enforces smooth single-atom dominance (R1 and R2), ensuring that when a query point is very close to an atom, that atom's contribution dominates while transitions remain smooth. The tanh magnitude term provides bounded, monotonic scaling (R3) and maintains non-vanishing gradients at moderate distances (R4). Together they satisfy all four design requirements with minimal sensitivity to hyperparameters.

\subsection{Limitations of Alternative Field Formulations}
Existing formulations systematically violate at least one of the four design requirements. Inverse-square weighting leads to unbounded gradients near atoms (violating R3); softmax normalization suffers from discontinuous transitions and cancellation in symmetric multi-atom settings (violating R2); Gaussian decay and related hybrids introduce rapidly vanishing gradients at moderate distances (violating R4) and fail to ensure local single-atom dominance (violating R1). These limitations motivate the composite design adopted in this work, which addresses all four requirements through the decoupled softmax--tanh formulation.

\subsection{Comparative Summary}
Figure~\ref{fig:field_methods} and Table~\ref{tab:field_comparison} summarize the qualitative differences among representative formulations, highlighting that the Softmax--Tanh field is the only approach satisfying all four design requirements simultaneously.
\begin{table}[t]
\centering
\caption{Comparison of gradient field formulations with respect to the design requirements. R1: Local single-atom dominance; R2: Smooth gradient transitions; R3: Bounded gradient magnitude; R4: Long-range gradient presence.}
\label{tab:field_comparison}
\begin{tabular}{lcccc}
\toprule
Method & R1 & R2 & R3 & R4 \\
\midrule
Gaussian Field & \texttimes & \checkmark & \checkmark & \texttimes \\
Softmax Field & \sim & \texttimes & \checkmark & \checkmark \\
Inverse Square Law & \checkmark & \checkmark & \texttimes & \checkmark \\
Gaussian Magnitude & \checkmark & \checkmark & \checkmark & \texttimes \\
Distance with Clipping & \checkmark & \texttimes & \checkmark & \checkmark \\
Softmax--Tanh (Ours) & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Adaptive Spatial Sampling}
\label{sec:adaptive_spatial_sampling}

When converting a molecular structure into a continuous gradient field, a critical practical consideration is how to sample query points in space. Naively sampling field values on a dense, uniform 3D grid is computationally expensive and largely inefficient: most grid points are either far away from all atoms or lie in regions where the gradient magnitude is negligible and provides little useful learning signal.

To address this issue, we adopt an \emph{adaptive spatial sampling} strategy that concentrates samples in regions that are both geometrically and physically informative, while still maintaining global spatial coverage. The key principle is to allocate more query points near atomic centers—where the gradient field changes rapidly and strongly influences reconstruction—while using fewer samples in distant regions.

\paragraph{Sampling strategy.}
Given a molecule with atom positions $\{\mathbf{x}_i\}_{i=1}^N$, we construct the set of query points $\mathcal{Q}$ as a mixture of two complementary distributions:
\begin{enumerate}
    \item \textbf{Atom-centered local sampling.}
    For each atom $i$, we sample points from an isotropic Gaussian distribution centered at $\mathbf{x}_i$:
    \[
        \mathbf{q} \sim \mathcal{N}(\mathbf{x}_i, \sigma_{\text{local}}^2 \mathbf{I}),
    \]
    where $\sigma_{\text{local}}$ controls the spatial extent of the local neighborhood. This component ensures dense coverage in regions where the field is expected to exhibit high curvature and strong gradients.

    \item \textbf{Global uniform sampling.}
    To preserve awareness of long-range interactions and background behavior, we additionally sample points uniformly from a bounding box that encloses the entire molecule with a fixed margin:
    \[
        \mathbf{q} \sim \text{Uniform}(\mathcal{B}),
    \]
    where $\mathcal{B}$ is defined by the minimum and maximum atomic coordinates expanded by a constant padding.
\end{enumerate}

The final query set $\mathcal{Q}$ is obtained by combining these two sources with a fixed ratio. In practice, we find that allocating the majority of samples to atom-centered regions yields significantly better reconstruction stability, while a small fraction of global samples is sufficient to regularize the field behavior away from atoms.

\paragraph{Benefits and motivation.}
This adaptive sampling scheme offers several advantages:
\begin{itemize}
    \item \textbf{Improved signal efficiency.}
    By focusing samples near atoms, the model observes stronger and more informative gradients, avoiding wasted computation on near-zero regions.
    \item \textbf{Stable optimization.}
    Dense local sampling reduces aliasing effects and improves the accuracy of gradient-based position updates during reconstruction.
    \item \textbf{Scalability.}
    The total number of query points can be kept moderate even for larger molecules, as sampling density adapts to molecular geometry rather than spatial volume.
\end{itemize}

Overall, adaptive spatial sampling provides an effective compromise between computational efficiency and field fidelity, and plays a crucial role in enabling stable and accurate field-to-molecule reconstruction.

\subsection{Field-to-Molecule Reconstruction Algorithm}
\label{sec:field_to_molecule_algorithm}

The reconstruction procedure converts a continuous vector field into discrete atomic coordinates through iterative gradient ascent followed by clustering.
For each atom type $k$, we initialize candidate points uniformly in the bounding box and iteratively update their positions by following the gradient field until convergence.
Converged points are then clustered using DBSCAN to extract atomic coordinates.

\begin{algorithm}[t]
\caption{Field-to-Molecule Reconstruction}
\label{algo:field_to_molecule}
\begin{algorithmic}[1]
\REQUIRE Vector field $\mathbf{V} \in \mathbb{R}^{m \times K \times 3}$, decoder $D_\psi$, latent codes $\mathbf{z}$, step size $\eta$, convergence threshold $\tau$, DBSCAN parameters $(\epsilon, m_{\text{min}})$
\ENSURE Atomic coordinates $\mathbf{X} \in \mathbb{R}^{n \times 3}$, atom types $T \in \{0, \ldots, K-1\}^n$
\STATE Initialize candidate points $\mathcal{P} \leftarrow \emptyset$
\FOR{each atom type $k \in \{0, \ldots, K-1\}$}
    \STATE Initialize $n_k$ candidate points $\{\mathbf{p}_i^{(0)}\}_{i=1}^{n_k}$ uniformly in bounding box
    \FOR{iteration $t = 1$ \TO $n_{\text{iter}}$}
        \STATE Compute field at current points: $\mathbf{v}_k(\mathbf{p}_i^{(t-1)}) \leftarrow D_\psi(\{\mathbf{p}_i^{(t-1)}\}, \mathbf{z})[i, k, :]$
        \FOR{each point $\mathbf{p}_i^{(t-1)}$}
            \IF{$\|\mathbf{v}_k(\mathbf{p}_i^{(t-1)})\| < \tau$}
                \STATE Mark point as converged
                \STATE \textbf{break}
            \ENDIF
            \STATE $\mathbf{p}_i^{(t)} \leftarrow \mathbf{p}_i^{(t-1)} + \eta \cdot \mathbf{v}_k(\mathbf{p}_i^{(t-1)})$
        \ENDFOR
    \ENDFOR
    \STATE Add converged points to $\mathcal{P}$ with type $k$
\ENDFOR
\STATE Cluster points in $\mathcal{P}$ using DBSCAN with distance threshold $\epsilon$ and minimum samples $m_{\text{min}}$
\STATE Extract cluster centers as atomic coordinates $\mathbf{X}$
\STATE Assign atom types $T$ based on point types
\STATE Infer chemical bonds using distance-based heuristics
\STATE Filter invalid structures based on bond length and angle constraints
\RETURN $(\mathbf{X}, T)$
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/clustering_evolution.png}
\caption{
\textbf{Visualization of the clustering process during field-to-molecule reconstruction.}
Examples are trained on the QM9 dataset.
The figure illustrates how candidate points, initialized uniformly in the bounding box, evolve through iterative gradient ascent and subsequently form distinct clusters via DBSCAN.
\textbf{Left to right:} Each column represents a successive iteration of the clustering process.
\textbf{Top and bottom rows:} Two independent examples of the reconstruction procedure.
Initially (leftmost column), points are randomly distributed throughout the 3D space.
As iterations progress, points converge toward atomic positions by following the gradient field, gradually forming dense clusters.
In the final state (rightmost column), well-separated clusters emerge, with each cluster center corresponding to a recovered atomic coordinate.
The color coding distinguishes different atom types: green represents carbon (C), gray represents hydrogen (H), red represents oxygen (O), blue represents nitrogen (N), and pink represents fluorine (F).
}
\label{fig:clustering_evolution}
\end{figure}

\section{Autoencoder Architecture and Analysis}\label{app:autoencoder}
This section provides additional architectural details and experimental analysis of the neural field autoencoder.

\subsection{Encoder Design: Cutoff and Gaussian Smearing}
The encoder maps discrete molecular graphs to latent grid codes using a GNN equipped with distance-based cutoff and Gaussian smearing. This design is particularly important due to the regular lattice structure of grid anchor points. Without smoothing, small variations in the cutoff radius can cause abrupt changes in neighborhood size, leading to discontinuous latent representations. Gaussian smearing ensures that messages vary smoothly with distance, yielding well-conditioned latent codes.

\subsection{Decoder Design: EGNN-Based Field Generation}
The decoder takes latent codes and continuous query points as input and predicts vector field values using an EGNN. By explicitly conditioning on query coordinates, the decoder defines a continuous, geometry-aware neural field that is equivariant to rigid transformations.

\subsection{Loss Imbalance in Auto-Encoding} \label{sec:loss_imbalance}
A central challenge in training the auto-encoder arises from the intrinsic imbalance among different loss components. Specifically, reconstruction objectives defined on atomic positions, field values, and auxiliary regularization terms operate at distinct spatial scales and exhibit substantially different gradient magnitudes.

Let $\mathcal{L}_{\text{pos}}$ denote the atom-wise coordinate reconstruction loss, $\mathcal{L}_{\text{field}}$ the regression loss on sampled vector field values, and $\mathcal{L}_{\text{reg}}$ additional smoothness or magnitude regularizers. A naive weighted sum
\begin{equation}
\mathcal{L} = \lambda_{\text{pos}} \mathcal{L}_{\text{pos}} + \lambda_{\text{field}} \mathcal{L}_{\text{field}} + \lambda_{\text{reg}} \mathcal{L}_{\text{reg}}
\end{equation}
leads to unstable optimization: gradients from densely sampled field supervision can dominate training, while sparse but semantically critical position errors receive insufficient emphasis.

This imbalance is further exacerbated by the spatial nature of the representation. Field supervision is applied to a large number of query points, many of which lie in regions of small magnitude, whereas coordinate reconstruction depends on a small set of atomic targets. As a result, minimizing $\mathcal{L}_{\text{field}}$ alone does not guarantee accurate atomic recovery.

To mitigate this issue, we normalize field losses by the number of effective query points and employ distance-based loss weighting to ensure comparable gradient scales across objectives. Specifically, for each query point $\mathbf{q}_j$, we assign a weight proportional to the exponential decay of its distance to the nearest ground truth atom:
\begin{equation}
w_j^{\text{field}} = \exp\left(-\min_i \|\mathbf{q}_j - \mathbf{x}_i\|\right),
\end{equation}
where $\mathbf{x}_i$ are the ground truth atomic coordinates. This weighting scheme emphasizes field supervision near atomic centers, where gradients are most informative, while reducing the influence of distant query points with negligible gradients. The field loss is then computed as:
\begin{equation}
\mathcal{L}_{\text{field}} = \frac{1}{\sum_j w_j^{\text{field}}} \sum_j w_j^{\text{field}} \cdot \ell_{\text{field}}(\mathbf{q}_j),
\end{equation}
where $\ell_{\text{field}}(\mathbf{q}_j)$ is the field reconstruction loss at query point $\mathbf{q}_j$. In addition, we found it beneficial to prioritize coordinate reconstruction during early training, gradually increasing the influence of field-level supervision. This strategy encourages the latent representation to first capture coarse molecular geometry before modeling fine-grained field structure.

\subsection{Additional Experiments on Autoencoder Performance}
We conduct two sets of experiments to evaluate autoencoder performance.

\paragraph{Field Reconstruction Quality.}
We measure the fidelity of reconstructed vector fields using mean squared error (MSE) and peak signal-to-noise ratio (PSNR). Since the gradient field encodes both direction and magnitude, we evaluate both the three-dimensional vector components and the field magnitude separately. Quantitative results are reported in Table~\ref{tab:field_recon}.

\begin{table}[t]
\centering
\caption{Field reconstruction quality metrics on QM9 and Drugs datasets. MSE measures the mean squared error between predicted and ground truth vector fields, while PSNR (peak signal-to-noise ratio) provides a logarithmic scale measure of reconstruction fidelity.}
\label{tab:field_recon}
\begin{tabular}{lcc}
\toprule
Metric & QM9 & Drugs \\
\midrule
Vector Field MSE & -- & -- \\
Vector Field Magnitude MSE & -- & -- \\
Vector Field PSNR (dB) & -- & -- \\
Vector Field Magnitude PSNR (dB) & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Molecule Reconstruction Accuracy.}
Using the decoded fields, we reconstruct molecular structures from the training set and evaluate reconstruction success rate and RMSD. We observe a 100\% reconstruction success rate with low RMSD (Table~\ref{tab:mol_recon}), demonstrating that the autoencoder successfully learns to encode and decode molecular structures in the neural field representation.

\begin{table}[t]
\centering
\caption{Molecule reconstruction accuracy on the training set for QM9 and Drugs datasets. Reconstruction success rate measures the percentage of molecules successfully reconstructed, while RMSD (root mean square deviation) quantifies the geometric accuracy of reconstructed atomic coordinates.}
\label{tab:mol_recon}
\begin{tabular}{lcc}
\toprule
Metric & QM9 & Drugs \\
\midrule
Reconstruction Success Rate (\%) & -- & -- \\
RMSD (\AA) & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Latent Noise Robustness Analysis}
To assess robustness, we inject isotropic Gaussian noise into latent codes,
\begin{equation}
\tilde{\mathbf{z}} = \mathbf{z} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 \mathbf{I}),
\end{equation}
and decode molecules from $\tilde{\mathbf{z}}$. We evaluate molecular validity, stability, valency, atom and bond statistics, and Wasserstein distances of bond length and angle distributions. Figure~\ref{fig:noise_robustness} illustrates how molecular stability and the distance between the distribution of bond angles change as we increasingly add noise to the latent codes. Each data point averages results over more than 4000 validation samples. Interestingly, the neural field is quite robust to noise: the metrics remain relatively stable even at moderate noise levels ($\sigma \approx 0.5$), and degrade gracefully as noise increases further. This robustness indicates that the latent space learned by the autoencoder is well-conditioned and suitable for diffusion-based generation, as the denoiser can effectively learn to recover clean codes from noisy inputs.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{noise_robustness.png}
\caption{Robustness analysis of the neural field to noise in the latent code space on QM9 (red) and GEOM-drugs (blue). (a) Stable molecule percentage and (b) bond angle Wasserstein-1 distance as we increasingly add noise to the latent codes. Metrics are computed from 4000 generated samples on the validation reference set.}
\label{fig:noise_robustness}
\end{figure}


\section{Latent Diffusion Model: Design and Ablations}\label{app:diffusion}
\subsection{Prediction Target and Diffusion Schedule}
We adopt an $x_0$-prediction objective in latent space. Empirically, predicting clean latent codes leads to more stable training and better downstream molecular quality than predicting noise, which we attribute to the bounded and smooth nature of the learned latent manifold. We employ a cosine noise schedule following prior work, as detailed in the main text.

\subsection{EGNN Architecture and Radius Ablation}
\label{sec:egnn_radius_ablation}

We parameterize the reverse diffusion process in latent field space using an Equivariant Graph Neural Network (EGNN). Each latent field is represented by a set of anchor points with fixed spatial coordinates, where node features correspond to latent codes and edges are constructed based on spatial proximity.

\paragraph{Motivation for EGNN.}
Latent field representations preserve explicit geometric structure: although diffusion is performed in latent space, the underlying anchor points retain their positions in $\mathbb{R}^3$. This makes equivariance to rotations and translations a desirable inductive bias. EGNNs naturally enforce $\mathrm{SE}(3)$ equivariance while allowing flexible message passing conditioned on relative distances, making them well-suited for modeling spatially grounded latent dynamics.

\paragraph{Local Connectivity via Radius Graphs.}
Edges in the EGNN are constructed using a radius-based neighborhood: two anchor points are connected if their Euclidean distance is below a threshold $r$. This design encodes a locality prior, encouraging the denoising process at each anchor to depend primarily on nearby latent context rather than global interactions.

Formally, let $\mathcal{E}_r = \{(i,j) \mid \|\mathbf{x}_i - \mathbf{x}_j\| \le r\}$ denote the edge set induced by radius $r$, where $\mathbf{x}_i$ are fixed anchor coordinates. Message passing is restricted to $\mathcal{E}_r$ at all diffusion timesteps.

\paragraph{Radius Ablation Study.}
We empirically study the effect of the neighborhood radius $r$ on generative performance. Small radii enforce strong locality but may limit long-range coordination, while overly large radii approximate fully connected graphs and introduce unnecessary interactions.

Our ablation experiments evaluate multiple radius values under otherwise identical diffusion settings, where the radius is parameterized relative to the anchor spacing: $r = k \cdot \text{anchor\_spacing}$ for various values of $k$. Quantitative results on the QM9 dataset are reported in Table~\ref{tab:radius_ablation}. We observe that moderate radii achieve the best trade-off between stability and expressiveness: they preserve local consistency of latent fields while still allowing information to propagate across the structure over multiple layers. In contrast, very small radii lead to fragmented latent updates, whereas excessively large radii degrade sample quality and increase computational cost without clear benefits.

These results support the use of radius-based EGNNs as an effective and principled parameterization for latent diffusion over spatially structured fields.

\begin{table*}[t]
\centering
\caption{Ablation study of neighborhood radius $r$ (parameterized as multiples of anchor spacing) in the EGNN denoiser on the QM9 dataset. Performance metrics are evaluated on the validation set.}
\label{tab:radius_ablation}
\begin{tabular}{lccccccccc}
\toprule
$r / \text{anchor\_spacing}$ & stable & stable & valid & unique & valency & atom & bond & bond & bond \\
 & mol $\%_\uparrow$ & atom $\%_\uparrow$ & $\%_\uparrow$ & $\%_\uparrow$ & ${W_{1}}_\downarrow$ & $\mathrm{TV}_\downarrow$ & $\mathrm{TV}_\downarrow$ & len ${W_{1}}_\downarrow$ & ang ${W_{1}}_\downarrow$ \\
\midrule
$0.5$ & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
$0.75$ & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
$1.0$ & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
$1.25$ & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
$1.5$ & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Discussion.}
Importantly, the radius parameter does not need to be finely tuned. Performance remains stable across a reasonable range of values, suggesting that the model is not overly sensitive to this hyperparameter. This robustness further indicates that locality is a useful inductive bias rather than a brittle architectural constraint.

Although this ablation study was conducted exclusively on the QM9 dataset, we argue that the optimal ratio $r / \text{anchor\_spacing}$ is transferable to other molecular datasets. This is because the radius parameter controls connectivity in the latent code space, which is independent of the physical scale of molecules. The anchor spacing defines the resolution of the latent field representation, and the optimal radius-to-spacing ratio reflects the intrinsic correlation structure of the learned latent codes rather than molecular geometry. Since the latent representation abstracts away molecular size and focuses on local field patterns, the optimal connectivity pattern should generalize across datasets with different molecular size distributions.


\section{Additional Experimental and Implementation Details}\label{app:impl}
We provide additional implementation details, including network hyperparameters, training schedules, and computational cost, to facilitate reproducibility. All experiments are conducted with fixed random seeds, and model checkpoints are selected based on validation performance. Detailed settings are reported in Table~\ref{tab:impl_details}.


